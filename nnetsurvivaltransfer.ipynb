{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f691807-033a-4ee2-8056-73a909e9a876",
   "metadata": {},
   "source": [
    "## Nnet-survival PDAC Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a16a61-e4e2-49eb-8684-f94b8a0e407f",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc28a25-2868-4061-924f-117c547c9c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c619a1-ad5b-44a7-983d-23a5943f3604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636926e5-f7b0-48df-9bbc-4293b8bce9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import keras.optimizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow.keras.backend as K\n",
    "import keras_tuner\n",
    "import nnet_survival\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "# import lasagne\n",
    "import nnet_survival\n",
    "# import cox_nnet_v2 as cox_nnet\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from matplotlib.pyplot import figure\n",
    "from tensorflow import keras\n",
    "# from kerastuner_tensorboard_logger import (\n",
    "#     TensorBoardLogger,\n",
    "#     setup_tb  # Optional\n",
    "# )\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers, layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sksurv.metrics import concordance_index_censored,brier_score\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold, train_test_split\n",
    "from sksurv.util import Surv\n",
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.metrics import Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d30201-bd19-4832-a762-a4efe171f762",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import multi-cancer Data\n",
    "<p> For every file in folder open read inside and store in pandas dataframe </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672ba2ad-39e0-4344-a23c-7bd1c7ffc4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/mrnaDataUnscaled/ACC.csv\n",
      "Data/mrnaDataUnscaled/BLCA.csv\n",
      "Data/mrnaDataUnscaled/BRCA.csv\n",
      "Data/mrnaDataUnscaled/CESC.csv\n",
      "Data/mrnaDataUnscaled/CHOL.csv\n",
      "Data/mrnaDataUnscaled/COAD.csv\n",
      "Data/mrnaDataUnscaled/DLBC.csv\n",
      "Data/mrnaDataUnscaled/ESCA.csv\n",
      "Data/mrnaDataUnscaled/GBM.csv\n",
      "Data/mrnaDataUnscaled/HNSC.csv\n",
      "Data/mrnaDataUnscaled/KICH.csv\n",
      "Data/mrnaDataUnscaled/KIRC.csv\n",
      "Data/mrnaDataUnscaled/KIRP.csv\n",
      "Data/mrnaDataUnscaled/LAML.csv\n",
      "Data/mrnaDataUnscaled/LGG.csv\n",
      "Data/mrnaDataUnscaled/LIHC.csv\n",
      "Data/mrnaDataUnscaled/LUAD.csv\n",
      "Data/mrnaDataUnscaled/LUSC.csv\n",
      "Data/mrnaDataUnscaled/MESO.csv\n",
      "Data/mrnaDataUnscaled/OV.csv\n",
      "Data/mrnaDataUnscaled/PCPG.csv\n",
      "Data/mrnaDataUnscaled/PRAD.csv\n",
      "Data/mrnaDataUnscaled/READ.csv\n",
      "Data/mrnaDataUnscaled/SARC.csv\n",
      "Data/mrnaDataUnscaled/SKCM.csv\n",
      "Data/mrnaDataUnscaled/STAD.csv\n",
      "Data/mrnaDataUnscaled/TGCT.csv\n",
      "Data/mrnaDataUnscaled/THCA.csv\n",
      "Data/mrnaDataUnscaled/THYM.csv\n",
      "Data/mrnaDataUnscaled/UCEC.csv\n",
      "Data/mrnaDataUnscaled/UCS.csv\n",
      "Data/mrnaDataUnscaled/UVM.csv\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_acc.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_blca.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_brca.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_cesc.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_chol.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_coad.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_dlbc.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_esca.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_gbm.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_hnsc.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_kich.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_kirc.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_kirp.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_laml.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_lgg.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_lihc.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_luad.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_lusc.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_meso.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_ov.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_pcpg.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_prad.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_read.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_sarc.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_skcm.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_stad.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_tgct.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_thca.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_thym.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_ucec.txt\n",
      "Data/ClinicalData/nationwidechildrens.org_clinical_patient_ucs.txt\n",
      "32\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# Create a new list mrna_list which contains pandas dataframes of each cancer datasets rna expression data\n",
    "# Create a new list clinical_list which contains clinical data for each cancer dataset\n",
    "\n",
    "mrna_list = []\n",
    "mrna_folder = \"Data/mrnaDataUnscaled\"\n",
    "clinical_folder = \"Data/ClinicalData\"\n",
    "for filename in sorted(os.listdir(mrna_folder)):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "    f = os.path.join(mrna_folder, filename)\n",
    "    print(f)\n",
    "    data = pd.read_csv(f, sep=\",\")\n",
    "    mrna_list.append(data)\n",
    "    \n",
    "clinical_list = []\n",
    "for filename in sorted(os.listdir(clinical_folder)):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    f = os.path.join(clinical_folder, filename)\n",
    "    print(f)\n",
    "    data = pd.read_csv(f, sep=\"\\t\")\n",
    "    clinical_list.append(data)\n",
    "\n",
    "print(len(mrna_list))\n",
    "print(len(clinical_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195bcaa-946c-4721-aa85-0d5a333843b8",
   "metadata": {},
   "source": [
    "### Prepare multi-cancer data for tensorflow \n",
    "<p>Every data set is handled individually, and merged in the end.<br></p>\n",
    "\n",
    "<ol>\n",
    "  <li>First The clinical samples are matched to the genomic samples.</li>\n",
    "  <li>Then samples are filtered out of the clinical data that are not in the mRNA data</li>\n",
    "  <li>After that all Not availables and Discrepancys are replaced with NaN.</li>\n",
    "  <li>Vital status Dead and Alive are set to 1 and 0 respectively.</li>\n",
    "  <li>Days are made numerical.</li>\n",
    "  <li>Remove patients that have time 0 or NaN for time/status.</li>\n",
    "  <li>Set Index of mRNA and clinical to patient barcode, and transpose the mRNA data so that the patients are in each row, and the genes in each column.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b22c1ce-96e4-4bc3-ad40-b20d3c431f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "x_list = []\n",
    "clinical_processed_list = []\n",
    "\n",
    "# Disable a warning we don't care about\n",
    "warn = pd.options.mode.chained_assignment\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "for i, mrna_clinical in enumerate(zip(mrna_list,clinical_list)):\n",
    "    print(i)\n",
    "    mrna, clinical = mrna_clinical\n",
    "    mrna_id = mrna.columns[1:]\n",
    "\n",
    "    # Match clinical samples to genomic samples\n",
    "    clinical.columns = clinical.iloc[0]\n",
    "    clinical = clinical.iloc[2:]\n",
    "\n",
    "    # Make intersection of patient id's that are in mrna and clinicaldata\n",
    "    clinical_id = clinical['bcr_patient_barcode']\n",
    "    intersection = list(set(mrna_id) & set(clinical_id))\n",
    "    intersection.sort()\n",
    "    intersection = pd.Series(intersection)\n",
    "\n",
    "\n",
    "    # Filter out samples in clinicaldata that are not in mrna\n",
    "    a = clinical['bcr_patient_barcode'].isin(intersection)\n",
    "    clinical = clinical[a]\n",
    "\n",
    "\n",
    "\n",
    "    # Create clinicaldata dataframe with the important features\n",
    "    clinicalnew = clinical[['bcr_patient_barcode',\n",
    "                                \"vital_status\",\n",
    "                                \"days_to_last_followup\",\n",
    "                                \"days_to_death\"]]\n",
    "    \n",
    "    #print(clinicalnew['vital_status'].value_counts())\n",
    "    \n",
    "\n",
    "    # Set missing data to NaN\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Discrepancy]\", 0)\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Not Available]\",np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Applicable]\", np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Available]\", np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Not Available]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Completed]\", np.nan)\n",
    "\n",
    "    # In vital_status set dead = 1 alive = 0\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"Dead\", 1)\n",
    "    clinicalnew[\"vital_status\"] = clinicalnew['vital_status'].replace(\"Alive\", 0)\n",
    "\n",
    "    # Set days to numeric values\n",
    "    clinicalnew[\"days_to_last_followup\"] = pd.to_numeric(clinicalnew[\"days_to_last_followup\"])\n",
    "    clinicalnew[\"days_to_death\"] = pd.to_numeric(clinicalnew[\"days_to_death\"])\n",
    "\n",
    "    # Combine days to death and days to last follow up to create a total time.\n",
    "    clinicalnew['time'] = clinicalnew['days_to_death'].combine_first(clinicalnew['days_to_last_followup'])\n",
    "\n",
    "    # Remove patients that have time 0 (so no follow up, just one recording)\n",
    "    clinicalnew = clinicalnew[clinicalnew.time != 0]\n",
    "\n",
    "    #Remove patients with nan for time or status\n",
    "    clinicalnew = clinicalnew.dropna(subset=['time'])\n",
    "    clinicalnew = clinicalnew.dropna(subset=['vital_status'])\n",
    "    \n",
    "    #Remove patients where time is negative\n",
    "    clinicalnew = clinicalnew[clinicalnew.time >= 0]\n",
    "\n",
    "    mrna.rename(columns={'Unnamed: 0': 'bcr_patient_barcode'}, inplace=True)\n",
    "    mrna = mrna.set_index('bcr_patient_barcode')\n",
    "\n",
    "    mrna = mrna.transpose()\n",
    "    mrna = mrna[mrna.index.isin(clinicalnew['bcr_patient_barcode'])]\n",
    "    mrna = mrna.reindex(np.random.RandomState(seed=1).permutation(mrna.index))\n",
    "    \n",
    "    clinicalnew = clinicalnew.set_index('bcr_patient_barcode')\n",
    "    clinicalnew = clinicalnew.loc[~clinicalnew.index.duplicated(), :]\n",
    "    clinicalnew = clinicalnew.reindex(index=mrna.index)\n",
    "    \n",
    "    x_list.append(mrna)\n",
    "    clinical_processed_list.append(clinicalnew)\n",
    "\n",
    "pd.options.mode.chained_assignment = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "586aba9f-d251-48c1-b0a1-60e29372fee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate the list of processed RNA-seq pandas dataframes together to one large dataset\n",
    "# Drop the NA values\n",
    "# Concatenate the list of processed Clinical pandas dataframes together to one large dataset\n",
    "\n",
    "xf_full = pd.concat(x_list)\n",
    "xf_full = xf_full.dropna(axis=1)\n",
    "clinicalf_full = pd.concat(clinical_processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b3230d-1b9a-428b-bf7f-3721cfd44827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmrna_list = []\\nmrna_folder = \"Data/TipData\"\\nclinical_folder = \"Data/TipClinical\"\\nfor filename in sorted(os.listdir(mrna_folder)):\\n    f = os.path.join(mrna_folder, filename)\\n    print(f)\\n    data = pd.read_csv(f, sep=\",\")\\n    mrna_list.append(data)\\n    \\nclinical_list = []\\nfor filename in sorted(os.listdir(clinical_folder)):\\n    f = os.path.join(clinical_folder, filename)\\n    data = pd.read_csv(f, sep=\",\",header=0,index_col=1)\\n    clinical_list.append(data)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new list mrna_list which contains pandas dataframes of each cancer datasets rna expression data\n",
    "# Create a new list clinical_list which contains clinical data for each cancer dataset\n",
    "\"\"\"\n",
    "mrna_list = []\n",
    "mrna_folder = \"Data/TipData\"\n",
    "clinical_folder = \"Data/TipClinical\"\n",
    "for filename in sorted(os.listdir(mrna_folder)):\n",
    "    f = os.path.join(mrna_folder, filename)\n",
    "    print(f)\n",
    "    data = pd.read_csv(f, sep=\",\")\n",
    "    mrna_list.append(data)\n",
    "    \n",
    "clinical_list = []\n",
    "for filename in sorted(os.listdir(clinical_folder)):\n",
    "    f = os.path.join(clinical_folder, filename)\n",
    "    data = pd.read_csv(f, sep=\",\",header=0,index_col=1)\n",
    "    clinical_list.append(data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25cd4c40-0acb-4729-9461-1224dbec1249",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 31\n"
     ]
    }
   ],
   "source": [
    "print(len(mrna_list), len(clinical_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0a5ced-0a25-4254-b8e5-d51e0a4978bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_list = []\\nclinical_processed_list = []\\n\\n# for i, mrna_clinical in enumerate(zip(mrna_list,clinical_list)):\\nfor mrna, clinical in zip(mrna_list,clinical_list):\\n    mrna, clinical = mrna_clinical\\n    mrna.rename(columns={\\'Unnamed: 0\\': \\'patient_code\\'}, inplace=True)\\n    mrna = mrna.set_index(\\'patient_code\\')\\n    \\n    mrna = mrna.transpose()\\n\\n    \\n    \\n    \\n    clinical = clinical[[\\'7.1|Status (death)\\',\\n                        \\'OS_Days\\']]\\n    \\n    \\n    clinical[\\'7.1|Status (death)\\'] = clinical[\\'7.1|Status (death)\\'].replace(\"No\", 0)\\n    clinical[\\'7.1|Status (death)\\'] = clinical[\\'7.1|Status (death)\\'].replace(\"Yes\", 1)\\n\\n    \\n    clinical_id = clinical.index\\n    mrna_id = mrna.index\\n    intersection = list(set(mrna_id) & set(clinical_id))\\n    intersection.sort()\\n    intersection = pd.Series(intersection)\\n\\n\\n    # Filter out samples in clinicaldata that are not in mrna\\n    a = mrna.index.isin(intersection)\\n    mrna = mrna[a]\\n    \\n    b = clinical.index.isin(intersection)\\n    clinical = clinical[b]\\n          \\n    x_list.append(mrna)\\n    clinical_processed_list.append(clinical)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x_list = []\n",
    "clinical_processed_list = []\n",
    "\n",
    "# for i, mrna_clinical in enumerate(zip(mrna_list,clinical_list)):\n",
    "for mrna, clinical in zip(mrna_list,clinical_list):\n",
    "    mrna, clinical = mrna_clinical\n",
    "    mrna.rename(columns={'Unnamed: 0': 'patient_code'}, inplace=True)\n",
    "    mrna = mrna.set_index('patient_code')\n",
    "    \n",
    "    mrna = mrna.transpose()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    clinical = clinical[['7.1|Status (death)',\n",
    "                        'OS_Days']]\n",
    "    \n",
    "    \n",
    "    clinical['7.1|Status (death)'] = clinical['7.1|Status (death)'].replace(\"No\", 0)\n",
    "    clinical['7.1|Status (death)'] = clinical['7.1|Status (death)'].replace(\"Yes\", 1)\n",
    "\n",
    "    \n",
    "    clinical_id = clinical.index\n",
    "    mrna_id = mrna.index\n",
    "    intersection = list(set(mrna_id) & set(clinical_id))\n",
    "    intersection.sort()\n",
    "    intersection = pd.Series(intersection)\n",
    "\n",
    "\n",
    "    # Filter out samples in clinicaldata that are not in mrna\n",
    "    a = mrna.index.isin(intersection)\n",
    "    mrna = mrna[a]\n",
    "    \n",
    "    b = clinical.index.isin(intersection)\n",
    "    clinical = clinical[b]\n",
    "          \n",
    "    x_list.append(mrna)\n",
    "    clinical_processed_list.append(clinical)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8071479c-46d1-4f43-9c60-f8ff78e30541",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nxtip = pd.concat(x_list)\\nxtip = xtip.dropna(axis=1)\\nytip = pd.concat(clinical_processed_list)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "xtip = pd.concat(x_list)\n",
    "xtip = xtip.dropna(axis=1)\n",
    "ytip = pd.concat(clinical_processed_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db8b5d7-f2b6-45ed-a2c0-387f210d51d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ytip = ytip[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216d056d-6c5e-4a02-abb6-fcf5d0740017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#xtip = xtip.sort_index()\n",
    "#ytip = ytip.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e267c9-47cd-490d-86a4-f5a75b590544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(xtip.shape)\n",
    "#print(ytip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc9e613-749b-4ff2-a7ef-48c20abe010b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(xtip.index)\n",
    "#print(ytip.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc482697-f2cf-4fb7-a224-59efda17c9e1",
   "metadata": {},
   "source": [
    "### Import subset of cancers\n",
    "\n",
    "<p> These cancers are most similar to PDAC, and give a small increase in performance</p>\n",
    "<p> Same procedure as above </p>\n",
    "<ul>\n",
    "    <li>LUAD</li>\n",
    "  <li>STAD</li>\n",
    "  <li>CHOL</li>\n",
    "  <li>SARC</li>\n",
    "  <li>TGCT</li>\n",
    "    <li>COAD</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41151ea3-8c72-4441-969d-efce6ff9c603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/subsetMrna/CHOL.csv\n",
      "Data/subsetMrna/COAD.csv\n",
      "Data/subsetMrna/LUAD.csv\n",
      "Data/subsetMrna/SARC.csv\n",
      "Data/subsetMrna/STAD.csv\n",
      "Data/subsetMrna/TGCT.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a new list mrna_list which contains pandas dataframes of each cancer datasets rna expression data for the subset of cancers\n",
    "# Create a new list clinical_list which contains clinical data for each cancer dataset\n",
    "\n",
    "mrna_list = []\n",
    "mrna_folder = \"Data/subsetMrna\"\n",
    "clinical_folder = \"Data/subsetClinical\"\n",
    "for filename in sorted(os.listdir(mrna_folder)):\n",
    "    f = os.path.join(mrna_folder, filename)\n",
    "    print(f)\n",
    "    data = pd.read_csv(f, sep=\",\")\n",
    "    mrna_list.append(data)\n",
    "    \n",
    "clinical_list = []\n",
    "for filename in sorted(os.listdir(clinical_folder)):\n",
    "    f = os.path.join(clinical_folder, filename)\n",
    "    data = pd.read_csv(f, sep=\"\\t\")\n",
    "    clinical_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756631de-40f0-4efd-b788-30b2eb9e941d",
   "metadata": {},
   "source": [
    "### Prepare subset of cancer data for tensorflow \n",
    "\n",
    "<p> Same steps as described above </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c892cc3-d196-4ee2-95c0-4ef940346bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_list = []\n",
    "clinical_processed_list = []\n",
    "\n",
    "# Disable a warning we don't care about\n",
    "warn = pd.options.mode.chained_assignment\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "for mrna,clinical in zip(mrna_list,clinical_list):\n",
    "    mrna_id = mrna.columns[1:]\n",
    "\n",
    "    # Match clinical samples to genomic samples\n",
    "    clinical.columns = clinical.iloc[0]\n",
    "    clinical = clinical.iloc[2:]\n",
    "\n",
    "    # Make intersection of patient id's that are in mrna and clinicaldata\n",
    "    clinical_id = clinical['bcr_patient_barcode']\n",
    "    intersection = list(set(mrna_id) & set(clinical_id))\n",
    "    intersection.sort()\n",
    "    intersection = pd.Series(intersection)\n",
    "\n",
    "\n",
    "    # Filter out samples in clinicaldata that are not in mrna\n",
    "    a = clinical['bcr_patient_barcode'].isin(intersection)\n",
    "    clinical = clinical[a]\n",
    "\n",
    "\n",
    "\n",
    "    # Create clinicaldata dataframe with the important features\n",
    "    clinicalnew = clinical[['bcr_patient_barcode',\n",
    "                                \"vital_status\",\n",
    "                                \"days_to_last_followup\",\n",
    "                                \"days_to_death\"]]\n",
    "    \n",
    "    #print(clinicalnew['vital_status'].value_counts())\n",
    "    \n",
    "\n",
    "    # Set missing data to NaN\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Discrepancy]\", 0)\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Not Available]\",np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Applicable]\", np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Available]\", np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Not Available]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Completed]\", np.nan)\n",
    "\n",
    "    # In vital_status set dead = 1 alive = 0\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"Dead\", 1)\n",
    "    clinicalnew[\"vital_status\"] = clinicalnew['vital_status'].replace(\"Alive\", 0)\n",
    "\n",
    "    # Set days to numeric values\n",
    "    clinicalnew[\"days_to_last_followup\"] = pd.to_numeric(clinicalnew[\"days_to_last_followup\"])\n",
    "    clinicalnew[\"days_to_death\"] = pd.to_numeric(clinicalnew[\"days_to_death\"])\n",
    "\n",
    "    # Combine days to death and days to last follow up to create a total time.\n",
    "    clinicalnew['time'] = clinicalnew['days_to_death'].combine_first(clinicalnew['days_to_last_followup'])\n",
    "\n",
    "    # Remove patients that have time 0 (so no follow up, just one recording)\n",
    "    clinicalnew = clinicalnew[clinicalnew.time != 0]\n",
    "\n",
    "    #Remove patients with nan for time or status\n",
    "    clinicalnew = clinicalnew.dropna(subset=['time'])\n",
    "    clinicalnew = clinicalnew.dropna(subset=['vital_status'])\n",
    "    \n",
    "    #Remove patients where time is negative\n",
    "    clinicalnew = clinicalnew[clinicalnew.time >= 0]\n",
    "\n",
    "    mrna.rename(columns={'Unnamed: 0': 'bcr_patient_barcode'}, inplace=True)\n",
    "    mrna = mrna.set_index('bcr_patient_barcode')\n",
    "\n",
    "    mrna = mrna.transpose()\n",
    "    mrna = mrna[mrna.index.isin(clinicalnew['bcr_patient_barcode'])]\n",
    "    mrna = mrna.reindex(np.random.RandomState(seed=1).permutation(mrna.index))\n",
    "    \n",
    "    clinicalnew = clinicalnew.set_index('bcr_patient_barcode')\n",
    "    clinicalnew = clinicalnew.loc[~clinicalnew.index.duplicated(), :]\n",
    "    clinicalnew = clinicalnew.reindex(index=mrna.index)\n",
    "    \n",
    "    x_list.append(mrna)\n",
    "    clinical_processed_list.append(clinicalnew)\n",
    "    \n",
    "pd.options.mode.chained_assignment = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93ffa385-1e95-46a8-83c1-ac1747f1dd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate the list of processed RNA-seq pandas dataframes together to one large dataset\n",
    "# Drop the NA values\n",
    "# Concatenate the list of processed Clinical pandas dataframes together to one large dataset\n",
    "\n",
    "xf_sub = pd.concat(x_list)\n",
    "xf_sub = xf_sub.dropna(axis=1)\n",
    "clinicalf_sub = pd.concat(clinical_processed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c584a92-dfe9-47f8-b942-a68cbc6fe26f",
   "metadata": {},
   "source": [
    "### TCGA PDAC data preparation\n",
    "\n",
    "<p> Same steps as above </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb2550e7-8a53-4889-94de-f4a697efe9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load TCGA PDAC clinical and RNA-seq data\n",
    "\n",
    "clinicaldata = pd.read_csv(f\"Data/TargetDataUnscaled/nationwidechildrens.org_clinical_patient_paad.txt\", sep='\\t')\n",
    "mrna = pd.read_csv(f\"Data/TargetDataUnscaled/PAAD.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86df2724-f283-4e82-9f06-d45be1f6963c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable a warning we don't care about\n",
    "warn = pd.options.mode.chained_assignment\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "mrna_id = mrna.columns[1:].tolist()\n",
    "\n",
    "# Match clinical samples to genomic samples\n",
    "clinicaldata.columns = clinicaldata.iloc[0]\n",
    "clinicaldata = clinicaldata.iloc[2:]\n",
    "\n",
    "\n",
    "# Make intersection of patient id's that are in mrna and clinicaldata\n",
    "clinical_id = clinicaldata['bcr_patient_barcode']\n",
    "intersection = list(set(mrna_id) & set(clinical_id))\n",
    "intersection.sort()\n",
    "intersection = pd.Series(intersection)\n",
    "\n",
    "\n",
    "# Filter out samples in clinicaldata that are not in mrna\n",
    "a = clinicaldata['bcr_patient_barcode'].isin(intersection)\n",
    "clinicaldata = clinicaldata[a]\n",
    "\n",
    "\n",
    "# Create clinicaldata dataframe with the important features\n",
    "clinicalnew = clinicaldata[['bcr_patient_barcode',\n",
    "                            \"vital_status\",\n",
    "                            \"days_to_last_followup\",\n",
    "                            \"days_to_death\"]]\n",
    "\n",
    "\n",
    "\n",
    "# Set missing data to NaN\n",
    "clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Discrepancy]\", 0)\n",
    "clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Applicable]\", np.nan)\n",
    "clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Available]\", np.nan)\n",
    "clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Discrepancy]\", np.nan)\n",
    "clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Not Available]\", np.nan)\n",
    "clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "\n",
    "# In vital_status set dead = 1 alive = 0\n",
    "clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"Dead\", 1)\n",
    "clinicalnew[\"vital_status\"] = clinicalnew['vital_status'].replace(\"Alive\", 0)\n",
    "\n",
    "# Set days to numeric values\n",
    "clinicalnew[\"days_to_last_followup\"] = pd.to_numeric(clinicalnew[\"days_to_last_followup\"])\n",
    "clinicalnew[\"days_to_death\"] = pd.to_numeric(clinicalnew[\"days_to_death\"])\n",
    "\n",
    "# Combine days to death and days to last follow up to create a total time.\n",
    "clinicalnew['time'] = clinicalnew['days_to_death'].combine_first(clinicalnew['days_to_last_followup'])\n",
    "\n",
    "# Remove patients that have time 0 (so no follow up, just one recording)\n",
    "clinicalnew = clinicalnew[clinicalnew.time != 0]\n",
    "\n",
    "#Remove patients with nan for time\n",
    "clinicalnew = clinicalnew.dropna(subset=['time'])\n",
    "\n",
    "mrna.rename(columns={'Unnamed: 0': 'bcr_patient_barcode'}, inplace=True)\n",
    "mrna = mrna.set_index('bcr_patient_barcode')\n",
    "\n",
    "mrna = mrna.transpose()\n",
    "mrna = mrna[mrna.index.isin(clinicalnew['bcr_patient_barcode'])]\n",
    "mrna = mrna.reindex(np.random.RandomState(seed=1).permutation(mrna.index))\n",
    "clinicalnew = clinicalnew.set_index('bcr_patient_barcode')\n",
    "clinicalnew = clinicalnew.reindex(index=mrna.index)\n",
    "\n",
    "pd.options.mode.chained_assignment = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0589a1b4-1994-4304-aa31-5cd373de3680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xt = mrna\n",
    "yt = clinicalnew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ac774-d47e-48ec-b6b4-161da7854dac",
   "metadata": {},
   "source": [
    "### TCGA ICGC Data\n",
    "<p> Similar steps to above </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25d1a318-3722-4d20-b3bc-a84e2de226e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load ICGC clinical and mrna data\n",
    "\n",
    "clinicaldata = pd.read_csv(f\"Data/ICGCDataUnscaled/PDAC_ICGC_clinical.csv\", sep=',')\n",
    "mrna = pd.read_csv(f\"Data/ICGCDataUnscaled/PDAC_ICGC.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1c5fd80-b6a5-4308-826b-1264fd88a923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable a warning we don't care about\n",
    "warn = pd.options.mode.chained_assignment\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Create clinicaldata dataframe with the important features\n",
    "clinicalnew = clinicaldata[['icgc_donor_id',\n",
    "                            \"donor_vital_status\",\n",
    "                            \"donor_survival_time\"]]\n",
    "\n",
    "\n",
    "\n",
    "# Set missing data to NaN\n",
    "clinicalnew['donor_vital_status'] = clinicalnew['donor_vital_status'].replace(\"\", np.nan)\n",
    "\n",
    "# In vital_status set dead = 1 alive = 0\n",
    "clinicalnew['donor_vital_status'] = clinicalnew['donor_vital_status'].replace(\"deceased\", 1)\n",
    "clinicalnew[\"donor_vital_status\"] = clinicalnew['donor_vital_status'].replace(\"alive\", 0)\n",
    "\n",
    "# Set days to numeric values\n",
    "clinicalnew[\"donor_survival_time\"] = pd.to_numeric(clinicalnew[\"donor_survival_time\"])\n",
    "\n",
    "# Combine days to death and days to last follow up to create a total time.\n",
    "clinicalnew['time'] = clinicalnew['donor_survival_time']\n",
    "\n",
    "# Remove patients that have time 0 (so no follow up, just one recording)\n",
    "clinicalnew = clinicalnew[clinicalnew.time != 0]\n",
    "\n",
    "#Remove patients with nan for time\n",
    "clinicalnew = clinicalnew.dropna(subset=['time'])\n",
    "\n",
    "mrna.rename(columns={'Unnamed: 0': 'icgc_donor_id'}, inplace=True)\n",
    "mrna = mrna.set_index('icgc_donor_id')\n",
    "\n",
    "clinicalnew = clinicalnew.set_index('icgc_donor_id')\n",
    "\n",
    "pd.options.mode.chained_assignment = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3885bba0-4760-4116-83eb-07b6ded19e90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xv = mrna\n",
    "yv = clinicalnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "015acc1a-002e-4c39-9c37-a92858657920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xv = xv.drop(['DO49201'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0ba05-6965-4e14-84b3-6d513407bc67",
   "metadata": {},
   "source": [
    "### Match the genes in all 3 datasets to each other\n",
    "<p> We now have 3 datasets: A large multi-cancer dataset, a PDAC dataset, and a ICGC PDAC dataset. </p>\n",
    "<p>Here we make sure all datasets contain the same genes, and the order of the genes is the same </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64e12637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 17098)\n",
      "(8605, 13608)\n",
      "(1529, 15290)\n",
      "(72, 15364)\n"
     ]
    }
   ],
   "source": [
    "print(xt.shape)\n",
    "print(xf_full.shape)\n",
    "print(xf_sub.shape)\n",
    "print(xv.shape)\n",
    "# print(xtip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c255f8b-7773-4357-956c-ea3a9fe74c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the column names into transcript id's and genes again to make similar to ICGC data later on\n",
    "\n",
    "columns = xt.columns.str.split(' / ')\n",
    "correct_columns = []\n",
    "for column in columns:\n",
    "    correct_columns.append(column[0])\n",
    "\n",
    "xt.columns = correct_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e69245fa-e247-4fd0-8a35-878bf2d0f8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the column names into transcript id's and genes again to make similar to ICGC data later on\n",
    "\n",
    "columns = xf_full.columns.str.split(' / ')\n",
    "correct_columns = []\n",
    "for column in columns:\n",
    "    correct_columns.append(column[0])\n",
    "\n",
    "xf_full.columns = correct_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3010a4a8-7ee9-4afd-9213-f6de4d3b1f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the column names into transcript id's and genes again to make similar to ICGC data later on\n",
    "\n",
    "columns = xf_sub.columns.str.split(' / ')\n",
    "correct_columns = []\n",
    "for column in columns:\n",
    "    correct_columns.append(column[0])\n",
    "\n",
    "xf_sub.columns = correct_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b02dccf-6e0d-40b9-904e-eb95114d2dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the genes in the TCGA PDAC data that are not in the full 32 cancer dataset\n",
    "\n",
    "labels_to_drop = xt.columns.difference(xf_full.columns)\n",
    "xt = xt.drop(labels=labels_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e32046c-a9bc-4197-b71f-269bd5915fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngenes_to_drop = xtip.columns.difference(xf_full.columns)\\nxtip = xtip.drop(labels=genes_to_drop,axis=1)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "genes_to_drop = xtip.columns.difference(xf_full.columns)\n",
    "xtip = xtip.drop(labels=genes_to_drop,axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59925601-a7db-4a02-8a9a-dfa7e3789cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove all duplicated genes in the three datasets\n",
    "\n",
    "xt = xt.loc[:,~xt.columns.duplicated()].copy()\n",
    "xf_full = xf_full.loc[:,~xf_full.columns.duplicated()].copy()\n",
    "xf_sub = xf_sub.loc[:,~xf_sub.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "516e5df5-633c-4747-b182-4d5399646a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xtip = xtip.loc[:,~xtip.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69e9e7b1-6091-4c42-b86a-8f66c391ec18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop genes from datasets so that they are all similar in number\n",
    "\n",
    "xt = xt.drop(labels=(xt.columns.difference(xf_full.columns)),axis=1)\n",
    "xv = xv.drop(labels=(xv.columns.difference(xf_full.columns)),axis=1)\n",
    "xf_full = xf_full.drop(labels=(xf_full.columns.difference(xt.columns)),axis=1)\n",
    "xf_sub = xf_sub.drop(labels=(xf_sub.columns.difference(xt.columns)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbb9d7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xt = xt.drop(labels=(xt.columns.difference(xv.columns)),axis=1)\n",
    "xf_full = xf_full.drop(labels=(xf_full.columns.difference(xv.columns)),axis=1)\n",
    "xf_sub = xf_sub.drop(labels=(xf_sub.columns.difference(xv.columns)),axis=1)\n",
    "#xtip = xtip.drop(labels=(xtip.columns.difference(xv.columns)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a4e7829-a6fa-44d7-a87c-ea15a23f2153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop genes from TCGA data so they are similar to ICGC data\n",
    "\n",
    "xt = xt.drop(labels=(xt.columns.difference(xv.columns)),axis=1)\n",
    "xf_full = xf_full.drop(labels=(xf_full.columns.difference(xv.columns)),axis=1)\n",
    "xf_sub = xf_sub.drop(labels=(xf_sub.columns.difference(xv.columns)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c3fc53e-5be8-4d3d-a8fb-847f96346298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cols = xt.columns.union(xtip.columns)\n",
    "\n",
    "# xtip = xtip.reindex(cols, axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5bacb-551a-4d55-a684-ae225f063dc2",
   "metadata": {},
   "source": [
    "Check if number of features/genes is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab6fee5c-2ea4-4b0d-aa64-f4708742a204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 12739)\n",
      "(8605, 12739)\n",
      "(1529, 12739)\n",
      "(72, 12739)\n"
     ]
    }
   ],
   "source": [
    "print(xt.shape)\n",
    "print(xf_full.shape)\n",
    "print(xf_sub.shape)\n",
    "print(xv.shape)\n",
    "# print(xtip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68380cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#genes_to_drop = xtip.columns.difference(xt.columns)\n",
    "#xtip = xtip.drop(labels=genes_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db3b0f73-1487-4724-b88f-210c81a28ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the order of the genes to how to they occur in the TCGA PDAC data\n",
    "\n",
    "xv = xv[xt.columns]\n",
    "xf_full = xf_full[xt.columns]\n",
    "xf_sub = xf_sub[xt.columns]\n",
    "#xtip = xtip[xt.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800cd7d-0b02-4298-8142-5ed8c4c98bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Source model\n",
    "<p> Start with setting pandas dataframes to numpy arrays </p>\n",
    "\n",
    "#### Do you want to get weights for experiment 3?\n",
    "<p> Choose the code block underneath to use the 32 cancer dataset as the source dataset </p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4518466-0894-484c-9c4d-017b8902c96f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set pandas dataframes to numpy arrays, also create numpy arrays from the time and vital status data\n",
    "# The large 32 cancer dataset is used here\n",
    "\n",
    "xf = xf_full.to_numpy()\n",
    "yf = clinicalf_full\n",
    "\n",
    "ytime = yf['time'].to_numpy()\n",
    "ystatus = yf['vital_status'].to_numpy()\n",
    "ystatusbool = yf['vital_status'].astype(bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec18990",
   "metadata": {},
   "source": [
    "#### Do you want to get weights for experiment 4?\n",
    "<p> Choose the code block underneath to use the subset of cancers similar to PDAC as the source dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc93a416-30ab-444d-b524-cb26f80b2e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set pandas dataframes to numpy arrays, also create numpy arrays from the time and vital status data\n",
    "# The subset of cancers similar to PDAC dataset is used here\n",
    "\"\"\"\n",
    "xf = xf_sub.to_numpy()\n",
    "yf = clinicalf_sub\n",
    "\n",
    "ytime = yf['time'].to_numpy()\n",
    "ystatus = yf['vital_status'].to_numpy()\n",
    "ystatusbool = yf['vital_status'].astype(bool)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2898fb-196b-4c98-962a-92f088d7d1ef",
   "metadata": {},
   "source": [
    "<p> Here we set our quantiles and make new labels that assign a quantile to each patient. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16ad9037-5460-4d78-a45c-b6fb1bda97e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create time quantiles to later be used in stratification\n",
    "\n",
    "first_quantile = np.quantile(ytime,0.25)\n",
    "second_quantile = np.quantile(ytime,0.5)\n",
    "third_quantile = np.quantile(ytime,0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d4f2895-c3b7-4520-9ee6-66449fcc5807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every patient's time value in ytime check in what quantile it falls and make a new list that contains the corresponding time quantile for each patient\n",
    "\n",
    "yquantile = []\n",
    "for time in ytime:\n",
    "    if time >= 0 and time < first_quantile:\n",
    "        yquantile.append(1)\n",
    "    elif time >= first_quantile and time < second_quantile:\n",
    "        yquantile.append(2)\n",
    "    elif time >= second_quantile and time < third_quantile:\n",
    "        yquantile.append(3)\n",
    "    elif time >= third_quantile:\n",
    "        yquantile.append(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6151ec8-bdc1-4d0e-93d2-d89932a01fb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set time interval for nnet survival custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200047eb-5180-4259-b8f1-1b2bf92c5cde",
   "metadata": {
    "tags": []
   },
   "source": [
    "><p>The time intervals should be set so that for every interval an equal amount of events occur. This is done with matplotlib and manually adjusting the bins untill you see an equal number of events per interval</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc13e476-75e0-41bb-b2ac-2eaa95874bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1910\n",
      "6695\n"
     ]
    }
   ],
   "source": [
    "# Check how many death events are in the data so you know how many death events you need per time interval\n",
    "\n",
    "number_of_events = (yf.vital_status == 1).sum()\n",
    "number_of_nonevents = (yf.vital_status == 0).sum()\n",
    "\n",
    "print(number_of_events)\n",
    "print(number_of_nonevents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4531a6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9219.0\n"
     ]
    }
   ],
   "source": [
    "print(yf['time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eac9e753-04b4-48b8-8ec8-79c882e5f05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([100.,  98.,  56., 100., 103.,  37.,  99.,  96.,  93.,  30.,  94.,\n",
       "         96.,  44.,  97.,  97.,  97.,  95.,  94.,  95.,  95.,  96.,  98.]),\n",
       " array([   0.,   55.,  100.,  135.,  180.,  240.,  260.,  322.,  370.,\n",
       "         430.,  455.,  520.,  580.,  620.,  730.,  840.,  970., 1140.,\n",
       "        1350., 1590., 1900., 2540., 9200.]),\n",
       " <BarContainer object of 22 artists>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a plot where you set the time interval bins in the bins variable, the plot should be as straight as possible, containing an equal number of death events per time interval\n",
    "\n",
    "test = yf.loc[yf['vital_status'] == 1]\n",
    "\n",
    "plt.hist(test['time'], bins=[0,55,100,135,180,240,260,322,370,430,455,520,580,620,730,840,970,1140,1350,1590,1900,2540,9200],ec='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e91a5-74b3-46e4-9797-dc38bc08ef0e",
   "metadata": {},
   "source": [
    "><p> Here we set our time intervals and create the survival matrix used by the nnet-survival model </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f8bf42e-6f86-4c60-b437-fc1bb0ee0b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we set the breaks for our time intervals which we obtained with the plot above\n",
    "# Use the first breaks for the large 32 cancer dataset, use the second breaks for the subset of cancers\n",
    "\n",
    "# breaks=np.asarray([0,55,100,135,180,240,260,322,370,430,455,520,580,620,730,840,970,1140,1350,1590,1900,2540,9200])\n",
    "breaks=np.asarray([0,40,98,153,180,240,280,330,370,428,495,580,660,750,860,990,1170,1400,1660,2100,7000])\n",
    "n_intervals = len(breaks)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea30958b-ff3a-4676-8bf4-fb887f8d386d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the survival array using the nnet_survival fuction 'make_surv_array'\n",
    "# This function takes the following input:\n",
    "#    ytime - numpy array of each patient's time until death/censor value\n",
    "#    ystatus - numpy array of each patient's survival status (1 for death event, 0 for alive/censored)\n",
    "#    breaks - the breaks for the time intervals that were found\n",
    "\n",
    "yf = nnet_survival.make_surv_array(ytime, ystatus, breaks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca6cb1-5358-4e6c-9e54-9fee5f986722",
   "metadata": {},
   "source": [
    "<p> This step adds the status and quantile labels to our survival matrix <br>\n",
    "This is used only with our 5-fold CV hyperparameter optimization</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f49ad0c5-2f30-40d2-9a6f-3db531919ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every survival matrix, status value, and quantile value merge the status and quantile values together and merge with the survival matrix.\n",
    "\n",
    "yf_final = ([],[])\n",
    "\n",
    "for matrix,status,quantile in zip(yf,ystatus,yquantile):\n",
    "    yf_final[0].append(matrix)\n",
    "    status_quantile = (str(int(status)) + \"_\" + str(quantile))\n",
    "    yf_final[1].append(status_quantile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15fec28b-6cb9-4601-8624-af0aa08b73aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2014c878-582f-4139-8d08-cd38b4cdbe7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Custom class for hypermodel + concordance index function ( ONLY USE IF YOU NEED TO FIND NEW HYPERPARAMETERS)\n",
    "<p> This code block shows 2 custom classes, one being a hypermodel that is built by our hyperparameter tuner, and where you can add variable parameters which will be tested. <br>\n",
    "The other is a custom hyper parameter tuner that utilizes cross validation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfd29e6a-4b2a-4bee-959c-715f0ff251ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SurvivalHyperModel(HyperModel):\n",
    "    def __init__(self, n_intervals, weights=[]):\n",
    "        self.n_intervals = n_intervals\n",
    "        self.weights = weights\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        hp_l2_value = hp.Choice('l2_value', values=[0.01,0.001,0.0001,0.00001])\n",
    "        model.add(Dense(np.sqrt(xf.shape[1]), input_dim=xf.shape[1], bias_initializer='zeros',activation='relu', kernel_regularizer=regularizers.l2(hp_l2_value)))\n",
    "        model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "        model.add(nnet_survival.PropHazards(self.n_intervals))\n",
    "        model.compile(loss=nnet_survival.surv_likelihood(self.n_intervals), optimizer=optimizers.Adam(learning_rate=0.00001),run_eagerly=False)\n",
    "        if self.weights:\n",
    "            model.layers[0].set_weights(self.weights)\n",
    "            return model\n",
    "        else:\n",
    "            return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,\n",
    "                         shuffle=False,\n",
    "                         **kwargs,\n",
    "                        )\n",
    "    \n",
    "class CVTuner(keras_tuner.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, x, y,epochs=1, *args, **kwargs):\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "        skf.get_n_splits(x,y[1])\n",
    "        val_losses = []\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Choice('batch_size', values=[8,16,32])\n",
    "        fold = 1\n",
    "        hp = trial.hyperparameters\n",
    "        for train_indices, test_indices in skf.split(x,y[1]):\n",
    "            early_stopping_hp = EarlyStopping(monitor='val_loss', patience=(100),min_delta=0.0005, restore_best_weights=True)\n",
    "            dir_for_logs = (\"Logs/ktuner/logs_pdac_21/\" + str(trial.trial_id) + \"/fold_\" + str(fold))\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=dir_for_logs, histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "            callbacks_list = [early_stopping_hp,tensorboard_callback]\n",
    "            x_train, x_test = x[train_indices], x[test_indices]\n",
    "            y_train, y_test = np.array(y[0])[train_indices], np.array(y[0])[test_indices]\n",
    "            y_train_labels, y_test_labels = np.array(y[1])[train_indices], np.array(y[1])[test_indices]\n",
    "            model = self.hypermodel.build(hp)\n",
    "            model.fit(x_train, y_train,validation_data=(x_test,y_test), batch_size=kwargs['batch_size'], epochs=epochs, callbacks=callbacks_list, verbose=0)       \n",
    "            y_pred = model.predict(x_test, verbose=0)\n",
    "            surv_prob=np.cumprod(y_pred, axis=1)[:,-1]\n",
    "            val_losses.append(model.evaluate(x_test, y_test))\n",
    "            fold += 1\n",
    "        self.oracle.update_trial(trial.trial_id, {'val_loss': np.mean(val_losses)})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0fec9e-0dc2-4273-ae32-dcdac067294b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameter optimization and training of the multi-cancer model (ONLY USE IF YOU NEED TO FIND NEW HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c1d1a-4d64-4604-975d-d1e0baf434b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p> In this section of code the hyperparameter tuning of the multi-cancer model occurs. <br>\n",
    "    The Hyperparameters are tuned on the whole datasets, and the results can be viewed with tensorboard. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b89c1-b01c-47b3-a802-570278fe83a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xf_t = scaler.fit_transform(xf)\n",
    "\n",
    "# logfile - Filename in which the results will be stored of the hyperparameter tuning.\n",
    "#           Call this file with Tensorboard to view loss curves and hyperparameter tuning results.\n",
    "#\n",
    "# name_of_project - Name of the Keras Tuner project.\n",
    "#                   Make sure this is different if you want to try a new search, else it will \n",
    "#                   just give you the results of the project previous project that had the same name !!!!\n",
    "#\n",
    "# name_of_dir - Directory where projects will be stored.\n",
    "\n",
    "logfile = \"Logs/ktuner/logs_pretrain_9_5/\"\n",
    "name_of_project = \"pretrain_model_9_5\"\n",
    "name_of_dir = \"pretrain_model_dir\"\n",
    "\n",
    "# The Cross validation tuner class, takes as input:\n",
    "#    hypermodel - A hypermodel class, which is your model but contains hyperparameter values for every parameter you want to test\n",
    "#    directory - Directory where the hyper parameter tuning project will be stored\n",
    "#    logger - Takes a TensorBoardLogger value as input which will store the loss of the model during training steps, \n",
    "#             which can be used in TensorBoard to create plots\n",
    "#    oracle - The keras tuner optimization method that you want to use, refer to: https://keras.io/api/keras_tuner/oracles/ for possible oracles.\n",
    "#             takes as input:\n",
    "#             objective - The objective of the hyperparameter tuning, and what determines whether a set of parameters is 'good'\n",
    "#             max_trials - The number of trials you wish to perform, each trial will contain a set of parameters to test.\n",
    "#\n",
    "# After creating the class, use .search to initiate the hyperparameter tuning, takes as input:\n",
    "#    xf_t - the scaled input features to train on\n",
    "#    yf_final - the label on which predictions will be assessed, contains the survival matrix and status_quantile for a patient\n",
    "#    epochs - the maximum number of epochs you wish to train (Early stopping is included in the class, so value is determined automatically)\n",
    "#    verbose - 1 to see progress of hyperparameter tuning in console, 0 to not see progress.\n",
    "                                                \n",
    "tuner = CVTuner(\n",
    "    hypermodel=SurvivalHyperModel(n_intervals),\n",
    "    project_name=name_of_project,\n",
    "    directory=name_of_dir,\n",
    "    logger = TensorBoardLogger(metrics=[\"val_loss\"], logdir=logfile),\n",
    "    oracle=kt.oracles.BayesianOptimization(\n",
    "        objective='val_loss',\n",
    "        max_trials=10))\n",
    "tuner.search(xf_t,yf_final,epochs=40000,verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0c26e-8a6a-41ea-a5cc-c4e1bd3b6265",
   "metadata": {},
   "source": [
    "<p> Now the source model is ran using the optimal parameters found in the previous step </p>\n",
    "\n",
    "#### Experiment 3/4 model running\n",
    "<p> This is the first block in experiment 3 or experiment 4, Figure 4 of manuscript. It depends on what source dataset you loaded into xf and yf earlier. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88425bf-8b48-4016-b40b-097f3937d4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "# Set the optimal parameters found for the source model\n",
    "\n",
    "optimal_l2 = 0.01\n",
    "optimal_batch = 16\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xf_t = scaler.fit_transform(xf)\n",
    "\n",
    "# Set the labels to the survival matrices\n",
    "\n",
    "yf = np.array(yf_final[0])\n",
    "\n",
    "# Create the model with the optimal parameters\n",
    "\n",
    "optimal_model = Sequential()\n",
    "optimal_model.add(Dense(np.sqrt(xf.shape[1]), input_dim=xf.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "optimal_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "optimal_model.add(nnet_survival.PropHazards(n_intervals))\n",
    "opt = optimizers.Adam(learning_rate=0.000001)\n",
    "optimal_model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=opt)\n",
    "\n",
    "\n",
    "# Create a tensorboard callback which saves the loss during the training of the model\n",
    "\n",
    "dir_for_log = (\"Logs/ktuner/source_model_full_paad_correct\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=dir_for_log, histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "# Fit the model to the input data xf_t and yf\n",
    "\n",
    "history = optimal_model.fit(xf_t,yf,batch_size=optimal_batch,epochs=1200, callbacks=callbacks_list,verbose=0)\n",
    "\n",
    "# Save the weights of the source model\n",
    "\n",
    "optimal_model.save_weights(\"Weights_new/source_model_subset\")   \n",
    "\n",
    "# Get performance of source model on full dataset which is very optimistic\n",
    "\n",
    "print(\"Performance model on full dataset (very optimistic):\")\n",
    "y_pred = optimal_model.predict(xf_t, verbose=0)\n",
    "oneyr_surv = np.cumprod(y_pred[:, 0:np.nonzero(breaks > 365)[0][0]], axis=1)[:, -1]\n",
    "c_index_test = concordance_index(ytime, oneyr_surv, ystatus)\n",
    "print(\"C_index: \" + str(c_index_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74a5e0-6b2e-49ae-b048-34a67921cf4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Data preperation TCGA PDAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c66b21b5-769c-464d-850a-bba1fca4d8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Set pandas dataframes of TCGA RNA-seq data to numpy array\n",
    "# Set pandas dataframes of TCGA clinical data (time and vital_status) to numpy arrays\n",
    "\n",
    "xt = xt.to_numpy()\n",
    "#\n",
    "ytime = yt['time'].to_numpy()\n",
    "ystatus = yt['vital_status'].to_numpy()\n",
    "ystatusbool = yt['vital_status'].astype(bool).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "806428f3-86aa-48d8-a9de-0b8728d909fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([17., 18., 16., 16., 15., 15., 19., 15., 18., 24.]),\n",
       " array([   0,   17,   70,  130,  180,  230,  300,  400,  580,  730, 2200]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a plot where you set the time interval bins in the bins variable, the plot should be as straight as possible, containing an equal number of death events per time interval\n",
    "\n",
    "test = yt.loc[yt['vital_status'] == 1]\n",
    "\n",
    "plt.hist(yt['time'], bins=[0,17,70,130,180,230,300,400,580,730,2200],ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10a6ee66-d274-4cb9-9ab0-1db528e4a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the breaks for our time intervals which we obtained with the plot above\n",
    "\n",
    "breaks=np.array([0,110,130,190,250,310,475,550,650,730,2200])\n",
    "n_intervals = len(breaks) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebd2499f-79f5-4138-b47f-1aa15447eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the survival array for the TCGA PDAC patients using the nnet_survival fuction 'make_surv_array'\n",
    "# This function takes the following input:\n",
    "#    ytime - numpy array of each patient's time until death/censor value\n",
    "#    ystatus - numpy array of each patient's survival status (1 for death event, 0 for alive/censored)\n",
    "#    breaks - the breaks for the time intervals that were found\n",
    "\n",
    "y_t = nnet_survival.make_surv_array(ytime, ystatus, breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8960c527-70b4-4b76-9049-d9db7163899b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create time quantiles to later be used in stratification\n",
    "\n",
    "first_quantile = np.quantile(ytime,0.25)\n",
    "second_quantile = np.quantile(ytime,0.5)\n",
    "third_quantile = np.quantile(ytime,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6df28d9c-9183-47d0-9165-52463e52e46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every patient's time value in ytime check in what quantile it falls and make a new list that contains the corresponding time quantile for each patient\n",
    "\n",
    "yquantile = []\n",
    "for time in ytime:\n",
    "    if time >= 0 and time < first_quantile:\n",
    "        yquantile.append(1)\n",
    "    elif time >= first_quantile and time < second_quantile:\n",
    "        yquantile.append(2)\n",
    "    elif time >= second_quantile and time < third_quantile:\n",
    "        yquantile.append(3)\n",
    "    elif time >= third_quantile:\n",
    "        yquantile.append(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55b13e65-b4a9-4c6b-83ba-b277798617af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every survival matrix, status value, and quantile value merge the status and quantile values together and merge with the survival matrix.\n",
    "\n",
    "yt_final = ([],[])\n",
    "\n",
    "for matrix,status,quantile in zip(y_t,ystatus,yquantile):\n",
    "    yt_final[0].append(matrix)\n",
    "    status_quantile = (str(int(status)) + \"_\" + str(quantile))\n",
    "    yt_final[1].append(status_quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e3dbe-e3c7-4527-a708-1b20467eac49",
   "metadata": {},
   "source": [
    "### Data preperation ICGC \n",
    "<p> Same steps as TCGA data, again make sure there are similar amounts of death events per time interval. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29e0b085-5737-4a38-aff8-e70ea1ac1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas dataframes of ICGC RNA-seq data to numpy array\n",
    "# Set pandas dataframes of ICGC clinical data (time and vital_status) to numpy arrays\n",
    "\n",
    "xv = xv.to_numpy()\n",
    "\n",
    "yvtime = yv['time'].to_numpy()\n",
    "yvstatus = yv['donor_vital_status'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12d37fd8-ce54-4449-9513-290f9e647a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array which contains the status and survival in days for each patient, this is used by the Cox-ph model as the labels\n",
    "\n",
    "ystatustime = []\n",
    "for status,time in zip(yvstatus,yvtime):\n",
    "    ystatustime.append((status,time))\n",
    "\n",
    "ysurvival_data = np.array(ystatustime, dtype=[('Status', '?'), ('Survival_in_days', '<f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ebea0-9034-42ec-a95d-3854111621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many death events are in the data so you know how many death events you need per time interval\n",
    "\n",
    "number_of_events = (yv['donor_vital_status'] == 1).sum()\n",
    "number_of_nonevents = (yv['donor_vital_status'] == 0).sum()\n",
    "\n",
    "print(number_of_events)\n",
    "print(number_of_nonevents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ad422-9661-4299-a6d7-527f599604f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a plot where you set the time interval bins in the bins variable, the plot should be as straight as possible, containing an equal number of death events per time interval\n",
    "\n",
    "test = yv.loc[yv['donor_vital_status'] == 1]\n",
    "\n",
    "# plt.hist(test['OS_Days'], bins=[0,165,230,260,345,390,425,460,600,1100,1880],ec='black')7189\n",
    "plt.hist(test['time'], bins=[0,18,100,120,132,150,173,177,190,400,2250],ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1372454-6e2c-4fc1-a8c7-73b18abe5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the breaks for our time intervals which we obtained with the plot above\n",
    "\n",
    "vbreaks = np.array([0,165,230,260,345,390,425,460,600,1100,1880])\n",
    "# vbreaks = np.array([0,18,100,120,132,150,173,177,190,400,2250])\n",
    "\n",
    "n_intervals = len(vbreaks) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1929b9dd-10f2-40be-a043-ddf8a0675646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the survival array for the ICGC PDAC patients using the nnet_survival fuction 'make_surv_array'\n",
    "# This function takes the following input:\n",
    "#    ytime - numpy array of each patient's time until death/censor value\n",
    "#    ystatus - numpy array of each patient's survival status (1 for death event, 0 for alive/censored)\n",
    "#    vbreaks - the breaks for the time intervals that were found\n",
    "\n",
    "y_v = nnet_survival.make_surv_array(yvtime, yvstatus, vbreaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3998e33f-a14c-4319-90d6-4d3ef9421263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time quantiles to later be used in stratification\n",
    "\n",
    "first_quantile_v = np.quantile(yvtime,0.25)\n",
    "second_quantile_v = np.quantile(yvtime,0.5)\n",
    "third_quantile_v = np.quantile(yvtime,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "426f383e-a1a6-4999-bac9-2091a64c70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every patient's time value in yvtime check in what quantile it falls and make a new list that contains the corresponding time quantile for each patient\n",
    "\n",
    "yvquantile = []\n",
    "for time in yvtime:\n",
    "    if time >= 0 and time < first_quantile_v:\n",
    "        yvquantile.append(1)\n",
    "    elif time >= first_quantile_v and time < second_quantile_v:\n",
    "        yvquantile.append(2)\n",
    "    elif time >= second_quantile_v and time < third_quantile_v:\n",
    "        yvquantile.append(3)\n",
    "    elif time >= third_quantile_v:\n",
    "        yvquantile.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23b17c52-a269-43c0-8b4f-0ed7a7e46471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every survival matrix, status value, and quantile value merge the status and quantile values together and merge with the survival matrix.\n",
    "\n",
    "yv_final = ([],[])\n",
    "\n",
    "for matrix,status,quantile in zip(y_v,yvstatus,yvquantile):\n",
    "    yv_final[0].append(matrix)\n",
    "    status_quantile = (str(int(status)) + \"_\" + str(quantile))\n",
    "    yv_final[1].append(status_quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1058a4-f714-4270-a0a2-16e69f42e959",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameter optimization and training of the TCGA PDAC model (ONLY RUN IF YOU NEED TO FIND NEW HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01262c13-0f0c-4266-836b-15aac375e605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xt = scaler.fit_transform(xt)\n",
    "\n",
    "# logfile - Filename in which the results will be stored of the hyperparameter tuning.\n",
    "#           Call this file with Tensorboard to view loss curves and hyperparameter tuning results.\n",
    "#\n",
    "# name_of_project - Name of the Keras Tuner project.\n",
    "#                   Make sure this is different if you want to try a new search, else it will \n",
    "#                   just give you the results of the project previous project that had the same name !!!!\n",
    "#\n",
    "# name_of_dir - Directory where projects will be stored.\n",
    "\n",
    "logfile = \"Logs/ktuner/logs_tcga_model/\"\n",
    "name_of_project = \"tcga_model\"\n",
    "name_of_dir = \"tcga_model_dir\"\n",
    "\n",
    "# The Cross validation tuner class, takes as input:\n",
    "#    hypermodel - A hypermodel class, which is your model but contains hyperparameter values for every parameter you want to test\n",
    "#    directory - Directory where the hyper parameter tuning project will be stored\n",
    "#    logger - Takes a TensorBoardLogger value as input which will store the loss of the model during training steps, \n",
    "#             which can be used in TensorBoard to create plots\n",
    "#    oracle - The keras tuner optimization method that you want to use, refer to: https://keras.io/api/keras_tuner/oracles/ for possible oracles.\n",
    "#             takes as input:\n",
    "#             objective - The objective of the hyperparameter tuning, and what determines whether a set of parameters is 'good'\n",
    "#             max_trials - The number of trials you wish to perform, each trial will contain a set of parameters to test.\n",
    "#\n",
    "# After creating the class, use .search to initiate the hyperparameter tuning, takes as input:\n",
    "#    xf_t - the scaled input features to train on\n",
    "#    yf_final - the label on which predictions will be assessed, contains the survival matrix and status_quantile for a patient\n",
    "#    epochs - the maximum number of epochs you wish to train (Early stopping is included in the class, so value is determined automatically)\n",
    "#    verbose - 1 to see progress of hyperparameter tuning in console, 0 to not see progress.\n",
    "                                                \n",
    "tuner = CVTuner(\n",
    "    hypermodel=SurvivalHyperModel(n_intervals),\n",
    "    project_name=name_of_project,\n",
    "    directory=name_of_dir,\n",
    "    logger = TensorBoardLogger(metrics=[\"val_loss\"], logdir=logfile),\n",
    "    oracle=kt.oracles.BayesianOptimization(\n",
    "        objective='val_loss',\n",
    "        max_trials=10))\n",
    "tuner.search(xt,yt_final,epochs=40000,verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15ca77-6487-4ac4-90b7-a0f484b31f17",
   "metadata": {},
   "source": [
    "### Run TCGA PDAC model just once and store weights\n",
    "### This is equal to first block in experiment 2, Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9109b96-8a04-4032-900a-560efb0d3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "xt_t = scaler.fit_transform(xt)\n",
    "\n",
    "# Set the optimal parameters found for the model\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "# Create the model with the optimal parameters\n",
    "model = Sequential()\n",
    "model.add(Dense(np.sqrt(xt.shape[1]), input_dim=xt.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "model.add(nnet_survival.PropHazards(n_intervals))\n",
    "model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Create a tensorboard callback which saves the loss during the training of the model\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_finetune_model\"), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "# Fit the model to the input data xt_t and y_t\n",
    "history = model.fit(xt_t,y_t,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0) \n",
    "\n",
    "# Save the weights of the model\n",
    "model.save_weights(\"Weights_new/pdac_only\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22776cc-fdb7-400e-b93b-3665145365a8",
   "metadata": {},
   "source": [
    "### Run TCGA PDAC Model with transferred weights from Multi-Cancer Model (Fine-tuning step)\n",
    "### This is equal to the second block for experiment 3, Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "770336c0-57d8-4404-960f-91ecc76eb9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "xt_t = scaler.fit_transform(xt)\n",
    "\n",
    "# Set the optimal parameters found for the model\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "# Create a model to initialize the weights of the source model into (should have the same architecture)\n",
    "init_model = Sequential()\n",
    "init_model.add(Dense(np.sqrt(xt.shape[1]), input_dim=xt.shape[1], bias_initializer='zeros', activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "init_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "init_model.add(nnet_survival.PropHazards(20))\n",
    "init_model.compile(loss=nnet_survival.surv_likelihood(20), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Load the weights of the source model into the initialization model\n",
    "init_model.load_weights(\"Weights_new/source_model_subset\")\n",
    "\n",
    "\n",
    "# Create a model to run the fine tuning step on (this uses the source model's weights)\n",
    "model = Sequential()\n",
    "model.add(Dense(np.sqrt(xt.shape[1]), input_dim=xt.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "model.add(nnet_survival.PropHazards(n_intervals))\n",
    "model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Transfer weights of source model's second hidden layer to fine tuning model's second hidden layer\n",
    "model.layers[1].set_weights(init_model.layers[1].get_weights())\n",
    "\n",
    "\n",
    "# Create a tensorboard callback which saves the loss during the training of the model\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_source_finetune_model_subset\"), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "# Fit the model to the input data xt_t and y_t\n",
    "history = model.fit(xt_t,y_t,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0) \n",
    "\n",
    "# Save the weights of the model\n",
    "model.save_weights(\"Weights_new/source_finetune_model_subset\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357fc7b-ab3e-4e18-a99a-3ac0fea2a3c4",
   "metadata": {},
   "source": [
    "### Train ICGC PDAC and validate with 5-fold CV\n",
    "### Depending on what experiment you want do the following:\n",
    "#### Experiment 1\n",
    "<p> Set transfer_learning to False, set name of results_file, run block </p>\n",
    "\n",
    "#### Experiment 2\n",
    "<p> Set transfer_learning to True, set name of results_file, load weights of model trained only on TCGA PDAC data at init_model.load_weights(\"path_to_weights\") </p>\n",
    "\n",
    "#### Experiment 3\n",
    "<p> Set transfer_learning to True, set name of results_file, load weights of model trained on all TCGA cancers, and TCGA PDAC data at init_model.load_weights(\"path_to_weights\") </p>\n",
    "\n",
    "#### Experiment 4\n",
    "<p> Set transfer_learning to True, set name of results_file, load weights of model trained on subset of TCGA cancers, and TCGA PDAC data at init_model.load_weights(\"path_to_weights\") </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3210d7c-00b8-4802-ba9d-1bf78d2a9453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1/50\n",
      "fold: 2/50\n",
      "fold: 3/50\n",
      "fold: 4/50\n",
      "fold: 5/50\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EF3FC3D558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "fold: 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EF3D9279D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "fold: 7/50\n",
      "fold: 8/50\n",
      "fold: 9/50\n",
      "fold: 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 11/50\n",
      "fold: 12/50\n",
      "fold: 13/50\n",
      "fold: 14/50\n",
      "fold: 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 16/50\n",
      "fold: 17/50\n",
      "fold: 18/50\n",
      "fold: 19/50\n",
      "fold: 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 21/50\n",
      "fold: 22/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22492\\3485481437.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m# Fit the model to the input data x_train_scaled and y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimal_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# Make predictions on the train data for the training performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2497\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1863\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Transferlearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "# Set transfer_learning to True, if you want transfer learning, else set it too False\n",
    "transfer_learning = True\n",
    "\n",
    "# File which will store the C-indexes obtained\n",
    "results_file = \"Results/cindexes_pdac_only\"\n",
    "\n",
    "cindex_list = []\n",
    "cindex_train_list = []\n",
    "\n",
    "total_cindexes = []\n",
    "total_cindexes_train = []\n",
    "\n",
    "# Set optimal parameters found with hyperparameter tuning\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "\n",
    "if transfer_learning == True:\n",
    "    # Create a model to initialize the weights of the source model into (should have the same architecture)\n",
    "    \n",
    "    init_model = Sequential()\n",
    "    init_model.add(Dense(np.sqrt(xt.shape[1]), input_dim=xt.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    init_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "    init_model.add(nnet_survival.PropHazards(10))\n",
    "    init_model.compile(loss=nnet_survival.surv_likelihood(10), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "    # Load the weights of the source model into the initialization model\n",
    "\n",
    "    init_model.load_weights(\"Weights_new/source_finetune_model_subset\")\n",
    "    \n",
    "\n",
    "# Create Repeated Stratified Kfold cross validation object\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=24)\n",
    "\n",
    "k = 1\n",
    "\n",
    "test_performance = []\n",
    "train_performance = []\n",
    "\n",
    "\n",
    "# Start the repeated 5 fold cross validation (will repeat 50 times, based on n_splits = 5 and n_repeats = 10)\n",
    "for train_indices, test_indices in rskf.split(xv,yv_final[1]):\n",
    "    print(\"fold: \" + str(k) + \"/50\")\n",
    "    \n",
    "    # Create splits of the input data\n",
    "    # Split the input features of the ICGC PDAC data\n",
    "    x_train, x_test = xv[train_indices], xv[test_indices]\n",
    "    # Split the input labels of the ICGC PDAC data (survival matrices)\n",
    "    y_train, y_test = np.array(yv_final[0])[train_indices], np.array(yv_final[0])[test_indices]\n",
    "    # Split the status_quantile variables for each patient\n",
    "    y_train_labels, y_test_labels = np.array(yv_final[1])[train_indices], np.array(yv_final[1])[test_indices]\n",
    "    # Split the status values for each patient\n",
    "    ystatus_train, ystatus_test = yvstatus[train_indices], yvstatus[test_indices]\n",
    "    # Split the time values for each patient\n",
    "    ytime_train, ytime_test = yvtime[train_indices], yvtime[test_indices]\n",
    "\n",
    "    # Scale the input train and test features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    # Create a model to run on the ICGC PDAC (target) data (this uses the source model's weights)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "    model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "    model.add(nnet_survival.PropHazards(n_intervals))\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.0000001))\n",
    "    \n",
    "    if transfer_learning == True:\n",
    "        # Transfer weights of source model's second hidden layer to fine tuning model's second hidden layer\n",
    "        model.layers[1].set_weights(init_model.layers[1].get_weights())\n",
    "        \n",
    "    # Create a tensorboard callback which saves the loss during the training of the model\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_pdac_only/fold_\" + str(k)), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "    callbacks_list = [tensorboard_callback]\n",
    "\n",
    "    # Fit the model to the input data x_train_scaled and y_train\n",
    "    history = model.fit(x_train_scaled,y_train,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0)\n",
    "\n",
    "    # Make predictions on the train data for the training performance\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_train_scaled,verbose=0), vbreaks, 365)\n",
    "    # Calculate C-index for train data using time, predictions and status values\n",
    "    c_index_train = concordance_index(ytime_train, y_pred, ystatus_train)\n",
    "\n",
    "    # Make predictions on the test data for test performance\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_test_scaled,verbose=0), vbreaks, 365)\n",
    "    # Calculate C-index for test data using time, predictions and status values\n",
    "    c_index_test = concordance_index(ytime_test, y_pred, ystatus_test)\n",
    "\n",
    "    # Store the C-index values in lists to write to .txt files\n",
    "    train_performance.append(c_index_train)\n",
    "    test_performance.append(c_index_test)\n",
    "\n",
    "    total_cindexes.append(c_index_test)\n",
    "    total_cindexes_train.append(c_index_train)\n",
    "\n",
    "    # Save results of current fold to file\n",
    "    with open(results_file, 'a') as o:\n",
    "        print(\"Fold: \" + str(k) + \"\\n\",file=o)\n",
    "        print(\"C-index train set: \" + str(c_index_train), file=o)\n",
    "        print(\"C-index test set: \" + str(c_index_test), file=o)\n",
    "        o.close()\n",
    "\n",
    "    k += 1\n",
    "\n",
    "# Save average C-index values and list of total C-indexes to .txt file\n",
    "with open(results_file, 'a') as o:\n",
    "    print(\"\\n\",file=o)\n",
    "    print(\"Average C-index train total: \" + str(sum(total_cindexes_train) / 50),file=o)\n",
    "    print(\"Average C-index test total: \" + str(sum(total_cindexes) / 50),file=o) \n",
    "    print(str(total_cindexes),file=o)\n",
    "    print(str(total_cindexes_train),file=o)\n",
    "    o.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654d772-02f8-43ee-a3b9-7b7c24e24b2d",
   "metadata": {},
   "source": [
    "### Cox-nnet hyperparameter tuning (Only run if you need new hyperparameters for cox-nnet comparison analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e7aab-d04c-4815-aa28-3cb29fec9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://garmiregroup.org/cox-nnet/docs/examples/ for an example on how to run Cox-nnet, and documentation\n",
    "\n",
    "model_params = dict(node_map = None, input_split = None)\n",
    "search_params = dict(method=\"adam\", learning_rate=0.01, momentum=0.9,\n",
    "                             max_iter=2000, stop_threshold=0.995, patience=1000, patience_incr=2, rand_seed=123,\n",
    "                             eval_step=23, lr_decay=0.9, lr_growth=1.0)\n",
    "cv_params = dict(cv_seed=1, n_folds=5, cv_metric=\"loglikelihood\", L2_range=np.arange(-4.5, 1, 0.5))\n",
    "\n",
    "likelihoods, L2_reg_params, mean_cvpl = cox_nnet.L2CVProfile(xv,yvtime,yvstatus,\n",
    "    model_params, search_params, cv_params, verbose=False)\n",
    "\n",
    "L2_reg = L2_reg_params[np.argmax(mean_cvpl)] #Best L2_reg is -5\n",
    "\n",
    "with open(\"Results/Optimal_L2_coxnnet\", 'a') as o:\n",
    "    print(\"Optimal L2 parameter:\\n\",file=o)\n",
    "    print(L2_reg,file=o)\n",
    "\n",
    "print(L2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4750c-4a6b-46d9-9445-dff647fa5d63",
   "metadata": {},
   "source": [
    "### Comparison Analysis\n",
    "<p> Compare Transfer learning model to Cox-PH and Cox-nnet model </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34fa82-6de9-4964-bb6d-79eacb0db05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = \"Results/Model_Comparison\"\n",
    "\n",
    "# Set optimal parameters found with hyperparameter tuning on TCGA PDAC data\n",
    "\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "# Create a model to initialize the weights of the source model into (should have the same architecture)\n",
    "\n",
    "init_model = Sequential()\n",
    "init_model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "init_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "init_model.add(nnet_survival.PropHazards(10))\n",
    "init_model.compile(loss=nnet_survival.surv_likelihood(10), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Load the weights of the source model into the initialization model\n",
    "\n",
    "init_model.load_weights(\"Weights/source_finetune_model_subset\")\n",
    " \n",
    "\n",
    "# Create Repeated Stratified Kfold cross validation object\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=24)\n",
    "\n",
    "k = 1\n",
    "\n",
    "train_performance_coxph = []\n",
    "test_performance_coxph = []\n",
    "\n",
    "train_performance_coxnnet = []\n",
    "test_performance_coxnnet = []\n",
    "\n",
    "train_performance_transfersnnet = []\n",
    "test_performance_transfersnnet = []\n",
    "\n",
    "# Start the repeated 5 fold cross validation (will repeat 50 times, based on n_splits = 5 and n_repeats = 10)\n",
    "for train_indices, test_indices in rskf.split(xv,yv_final[1]):\n",
    "    print(\"Loop: \" + str(k) + \"/50\")\n",
    "    \n",
    "    # Create splits of the input data\n",
    "    # Split the input features of the ICGC PDAC data\n",
    "    x_train, x_test = xv[train_indices], xv[test_indices]\n",
    "    # Split the input labels of the ICGC PDAC data (survival matrices)\n",
    "    y_train, y_test = np.array(yv_final[0])[train_indices], np.array(yv_final[0])[test_indices]\n",
    "    # Split the time values for each patient\n",
    "    yvtime_train, yvtime_test = yvtime[train_indices], yvtime[test_indices]\n",
    "    # Split the status values for each patient\n",
    "    yvstatus_train, yvstatus_test = yvstatus[train_indices], yvstatus[test_indices]\n",
    "    # Split the custom survival labels used by the cox-ph model\n",
    "    ysurvival_data_train, ysurvival_data_test = ysurvival_data[train_indices], ysurvival_data[test_indices]\n",
    "\n",
    "    \n",
    "    # Scale the input train and test features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    ### Cox-PH ###\n",
    "    \n",
    "    # Create the Cox-PH elastic net model\n",
    "    cox_elastic = CoxnetSurvivalAnalysis(l1_ratio=0.9, alpha_min_ratio=0.01)\n",
    "    \n",
    "    # Fit the model on the scaled train data and custom survival data\n",
    "    cox_elastic.fit(x_train_scaled,ysurvival_data_train)\n",
    "    \n",
    "    # Calculate the C-index on the train data\n",
    "    c_index_train = cox_elastic.score(x_train_scaled,ysurvival_data_train)\n",
    "    # Calculate the C-index on the test data\n",
    "    c_index_test = cox_elastic.score(x_test_scaled,ysurvival_data_test)\n",
    "    \n",
    "    # Add the C-index train score to train_performance_coxph\n",
    "    train_performance_coxph.append(c_index_train)\n",
    "    # Add the C-index test score to test_performance_coxph\n",
    "    test_performance_coxph.append(c_index_test)\n",
    "  \n",
    "    ### Cox-nnet ###\n",
    "\n",
    "    model_params = dict(node_map = None, input_split = None)\n",
    "    search_params = dict(method=\"adam\", learning_rate=0.01, momentum=0.9,\n",
    "                                 max_iter=2000, stop_threshold=0.995, patience=1000, patience_incr=2, rand_seed=123,\n",
    "                                 eval_step=23, lr_decay=0.9, lr_growth=1.0)\n",
    "    cv_params = dict(cv_seed=1, n_folds=5, cv_metric=\"loglikelihood\", L2_range=np.arange(-4.5, 1, 0.5))\n",
    "    \n",
    "    # Train model the model using the optimal L2 parameter found with the hyperparmaeter tuning\n",
    "    L2_reg = -4.5\n",
    "    model_params = dict(node_map = None, input_split = None, L2_reg=np.exp(L2_reg))\n",
    "    cox_nnet_model, cox_nnet_cost_iter = cox_nnet.trainCoxMlp(x_train_scaled, yvtime_train,yvstatus_train, model_params, search_params, verbose=False)\n",
    "    \n",
    "    # Make a prediction on the training data\n",
    "    cox_nnet_theta_train = cox_nnet_model.predictNewData(x_train_scaled)\n",
    "    # Make a prediction on the testing data\n",
    "    cox_nnet_theta_test = cox_nnet_model.predictNewData(x_test_scaled)\n",
    "\n",
    "    # Calculate train C-index from the time data, predictions made by cox-nnet and the patient status data, store the results in train_performance_coxnnet\n",
    "    train_performance_coxnnet.append(concordance_index(yvtime_train,-cox_nnet_theta_train,yvstatus_train))\n",
    "    # Calculate test C-index from the time data, predictions made by cox-nnet and the patient status data, store the results in test_performance_coxnnet\n",
    "    test_performance_coxnnet.append(concordance_index(yvtime_test,-cox_nnet_theta_test,yvstatus_test))\n",
    "\n",
    "    ### OUR MODEL ###\n",
    "    \n",
    "    # Create a model to run on the ICGC PDAC (target) data (this uses the source model's weights)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "    model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "    model.add(nnet_survival.PropHazards(10))\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "    \n",
    "    # Transfer weights of source model's second hidden layer to fine tuning model's second hidden layer\n",
    "    model.layers[1].set_weights(init_model.layers[1].get_weights())\n",
    "\n",
    "    # Create a tensorboard callback which saves the loss during the training of the model\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_comparison_new/fold_\" + str(k)), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "    callbacks_list = [tensorboard_callback]\n",
    "\n",
    "    # Fit the model to the input data x_train_scaled and y_train\n",
    "    history = model.fit(x_train_scaled,y_train,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0)\n",
    "\n",
    "    # Make predictions on the train data for the training performance\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_train_scaled,verbose=0), vbreaks, 365)\n",
    "    # Calculate C-index for train data using time, predictions and status values\n",
    "    c_index_train = concordance_index(yvtime_train, y_pred, yvstatus_train)\n",
    "    \n",
    "    # Make predictions on the test data for test performance\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_test_scaled,verbose=0), vbreaks, 365)\n",
    "    # Calculate C-index for test data using time, predictions and status values\n",
    "    c_index_test = concordance_index(yvtime_test, y_pred, yvstatus_test)\n",
    "\n",
    "    # Save the train C-index of our model to train_performance_transfersnnet\n",
    "    train_performance_transfersnnet.append(c_index_train)\n",
    "    # Save the test C-index of our model to test_performance_transfersnnet\n",
    "    test_performance_transfersnnet.append(c_index_test)\n",
    "    \n",
    "    # Save the average C-index  and total C-index list each approach every fold\n",
    "    with open(results_file, 'a') as o:\n",
    "        print(\"C-indexes, Cox-PH:\\n\",file=o)\n",
    "        print(\"Train:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(train_performance_coxph)/k),file=o)\n",
    "        print(str(train_performance_coxph),file=o)\n",
    "        print(\"Test:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(test_performance_coxph)/k),file=o)\n",
    "        print(str(test_performance_coxph),file=o)\n",
    "        print(\"\\n\",file=o)\n",
    "        print(\"C-indexes, Cox-nnet:\\n\",file=o)\n",
    "        print(\"Train:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(train_performance_coxnnet)/k),file=o)\n",
    "        print(str(train_performance_coxnnet),file=o)\n",
    "        print(\"Test:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(test_performance_coxnnet)/k),file=o)\n",
    "        print(str(test_performance_coxnnet),file=o)\n",
    "        print(\"\\n\",file=o)\n",
    "        print(\"C-indexes, our model:\\n\",file=o)\n",
    "        print(\"Train:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(train_performance_transfersnnet)/k),file=o)\n",
    "        print(str(train_performance_transfersnnet),file=o)\n",
    "        print(\"Test:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(test_performance_transfersnnet)/k),file=o)\n",
    "        print(str(test_performance_transfersnnet),file=o)\n",
    "        print(\"\\n\",file=o)\n",
    "    \n",
    "    o.close()\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fd526-7f65-4126-8319-67cfb95f44ab",
   "metadata": {},
   "source": [
    "### Test survival probabilities for patient 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "901fc562-0d2a-477b-bb16-52d9b7837407",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient = xv[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b7ce622-4a20-4751-8c62-a900df885933",
   "metadata": {},
   "outputs": [],
   "source": [
    "xv = np.delete(xv, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57f3aa95-e5e9-4f86-bcaa-fd25cd136f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient_y = yv_final[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01cd2334-2e8b-4a3c-b678-238bcb3a7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = list(yv_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13b55d63-3a3b-4ef4-ae2c-be1f692cc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.delete(y_train_all, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46859b4-39a7-45a4-a9db-74466a77fd5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the file that stores the survival probabilities\n",
    "\n",
    "results_file = \"Results/Model_surv_probabilities\"\n",
    "\n",
    "# Set optimal parameters found with hyperparameter tuning on TCGA PDAC data\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "# Create a model to initialize the weights of the source model into (should have the same architecture)\n",
    "\n",
    "init_model = Sequential()\n",
    "init_model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "init_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "init_model.add(nnet_survival.PropHazards(10))\n",
    "init_model.compile(loss=nnet_survival.surv_likelihood(10), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Load the weights of the source model into the initialization model\n",
    "\n",
    "init_model.load_weights(\"Weights/source_finetune_model_subset\")\n",
    " \n",
    "\n",
    "# Create Repeated Stratified Kfold cross validation object\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=24)\n",
    "\n",
    "k = 1\n",
    "\n",
    "surv_probablity = []\n",
    "\n",
    "# Set the prediction time you want to get the survival probability of\n",
    "\n",
    "pred_time = 456.25\n",
    "\n",
    "# Start the repeated 5 fold cross validation (will repeat 50 times, based on n_splits = 5 and n_repeats = 10)\n",
    "for _ in range(50):\n",
    "    print(\"Loop: \" + str(k) + \"/50\")\n",
    "    \n",
    "    # Scale the input train and test features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(xv)\n",
    "    x_test_scaled = scaler.transform(test_patient.reshape(1, -1))\n",
    "    # xv_ext_train.iloc[:,0:14242] = scaler.transform(xv_ext_train.iloc[:,0:14242])\n",
    "    # xv_ext_test.iloc[:,0:14242] = scaler.transform(xv_ext_test.iloc[:,0:14242])\n",
    "\n",
    "    # Create a model to run on the ICGC PDAC (target) data (this uses the source model's weights)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "    model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "    model.add(nnet_survival.PropHazards(10))\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "    \n",
    "    # Transfer weights of source model's second hidden layer to fine tuning model's second hidden layer\n",
    "    model.layers[1].set_weights(init_model.layers[1].get_weights())\n",
    "\n",
    "    # Create a tensorboard callback which saves the loss during the training of the model\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_pdac_icgc_testpatient/fold_\" + str(k)), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "    callbacks_list = [tensorboard_callback]\n",
    "\n",
    "    # Fit the model to the input data x_train_scaled and y_test_all\n",
    "    history = model.fit(x_train_scaled,y_train_all,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0)\n",
    "\n",
    "    # Predict the survival probability of patient 5 at set pred_time\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_test_scaled,verbose=0), vbreaks, pred_time)\n",
    "    \n",
    "    # Save survival probability to list\n",
    "    surv_probablity.append(y_pred)\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "# Store survival probabilities to file\n",
    "with open(results_file, 'a') as o:\n",
    "    print(\"Survival probabilities for \" + str(pred_time) + \" days:\\n\", file=o)\n",
    "    print(str(surv_probablity),file=o)\n",
    "    print(str(sum(surv_probablity)/50),file=o)\n",
    "    o.close()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
