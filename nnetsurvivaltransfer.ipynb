{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f691807-033a-4ee2-8056-73a909e9a876",
   "metadata": {},
   "source": [
    "## Nnet-survival PDAC Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a16a61-e4e2-49eb-8684-f94b8a0e407f",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "636926e5-f7b0-48df-9bbc-4293b8bce9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import keras.optimizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow.keras.backend as K\n",
    "import keras_tuner\n",
    "import nnet_survival\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "import lasagne\n",
    "import nnet_survival\n",
    "import cox_nnet_v2 as cox_nnet\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from matplotlib.pyplot import figure\n",
    "from tensorflow import keras\n",
    "from kerastuner_tensorboard_logger import (\n",
    "    TensorBoardLogger,\n",
    "    setup_tb  # Optional\n",
    ")\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers, layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sksurv.metrics import concordance_index_censored,brier_score\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold, train_test_split\n",
    "from sksurv.util import Surv\n",
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.metrics import Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d30201-bd19-4832-a762-a4efe171f762",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import multi-cancer Data\n",
    "<p> For every file in folder open read inside and store in pandas dataframe </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "672ba2ad-39e0-4344-a23c-7bd1c7ffc4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/mrnaDataUnscaled/ACC.csv\n",
      "Data/mrnaDataUnscaled/BLCA.csv\n",
      "Data/mrnaDataUnscaled/BRCA.csv\n",
      "Data/mrnaDataUnscaled/CESC.csv\n",
      "Data/mrnaDataUnscaled/CHOL.csv\n",
      "Data/mrnaDataUnscaled/COAD.csv\n",
      "Data/mrnaDataUnscaled/DLBC.csv\n",
      "Data/mrnaDataUnscaled/ESCA.csv\n",
      "Data/mrnaDataUnscaled/GBM.csv\n",
      "Data/mrnaDataUnscaled/HNSC.csv\n",
      "Data/mrnaDataUnscaled/KICH.csv\n",
      "Data/mrnaDataUnscaled/KIRC.csv\n",
      "Data/mrnaDataUnscaled/KIRP.csv\n",
      "Data/mrnaDataUnscaled/LAML.csv\n",
      "Data/mrnaDataUnscaled/LGG.csv\n",
      "Data/mrnaDataUnscaled/LIHC.csv\n",
      "Data/mrnaDataUnscaled/LUAD.csv\n",
      "Data/mrnaDataUnscaled/LUSC.csv\n",
      "Data/mrnaDataUnscaled/MESO.csv\n",
      "Data/mrnaDataUnscaled/OV.csv\n",
      "Data/mrnaDataUnscaled/PCPG.csv\n",
      "Data/mrnaDataUnscaled/PRAD.csv\n",
      "Data/mrnaDataUnscaled/READ.csv\n",
      "Data/mrnaDataUnscaled/SARC.csv\n",
      "Data/mrnaDataUnscaled/SKCM.csv\n",
      "Data/mrnaDataUnscaled/STAD.csv\n",
      "Data/mrnaDataUnscaled/TGCT.csv\n",
      "Data/mrnaDataUnscaled/THCA.csv\n",
      "Data/mrnaDataUnscaled/THYM.csv\n",
      "Data/mrnaDataUnscaled/UCEC.csv\n",
      "Data/mrnaDataUnscaled/UCS.csv\n",
      "Data/mrnaDataUnscaled/UVM.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a new list mrna_list which contains pandas dataframes of each cancer datasets rna expression data\n",
    "# Create a new list clinical_list which contains clinical data for each cancer dataset\n",
    "\n",
    "mrna_list = []\n",
    "mrna_folder = \"Data/mrnaDataUnscaled\"\n",
    "clinical_folder = \"Data/ClinicalData\"\n",
    "for filename in sorted(os.listdir(mrna_folder)):\n",
    "    f = os.path.join(mrna_folder, filename)\n",
    "    print(f)\n",
    "    data = pd.read_csv(f, sep=\",\")\n",
    "    mrna_list.append(data)\n",
    "    \n",
    "clinical_list = []\n",
    "for filename in sorted(os.listdir(clinical_folder)):\n",
    "    f = os.path.join(clinical_folder, filename)\n",
    "    data = pd.read_csv(f, sep=\"\\t\")\n",
    "    clinical_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195bcaa-946c-4721-aa85-0d5a333843b8",
   "metadata": {},
   "source": [
    "### Prepare multi-cancer data for tensorflow \n",
    "<p>Every data set is handled individually, and merged in the end.<br></p>\n",
    "\n",
    "<ol>\n",
    "  <li>First The clinical samples are matched to the genomic samples.</li>\n",
    "  <li>Then samples are filtered out of the clinical data that are not in the mRNA data</li>\n",
    "  <li>After that all Not availables and Discrepancys are replaced with NaN.</li>\n",
    "  <li>Vital status Dead and Alive are set to 1 and 0 respectively.</li>\n",
    "  <li>Days are made numerical.</li>\n",
    "  <li>Remove patients that have time 0 or NaN for time/status.</li>\n",
    "  <li>Set Index of mRNA and clinical to patient barcode, and transpose the mRNA data so that the patients are in each row, and the genes in each column.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b22c1ce-96e4-4bc3-ad40-b20d3c431f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "x_list = []\n",
    "clinical_processed_list = []\n",
    "\n",
    "for mrna,clinical in zip(mrna_list,clinical_list):\n",
    "    mrna_id = mrna.columns[1:]\n",
    "\n",
    "    # Match clinical samples to genomic samples\n",
    "    clinical.columns = clinical.iloc[0]\n",
    "    clinical = clinical.iloc[2:]\n",
    "\n",
    "    # Make intersection of patient id's that are in mrna and clinicaldata\n",
    "    clinical_id = clinical['bcr_patient_barcode']\n",
    "    intersection = list(set(mrna_id) & set(clinical_id))\n",
    "    intersection.sort()\n",
    "    intersection = pd.Series(intersection)\n",
    "\n",
    "\n",
    "    # Filter out samples in clinicaldata that are not in mrna\n",
    "    a = clinical['bcr_patient_barcode'].isin(intersection)\n",
    "    clinical = clinical[a]\n",
    "\n",
    "\n",
    "\n",
    "    # Create clinicaldata dataframe with the important features\n",
    "    clinicalnew = clinical[['bcr_patient_barcode',\n",
    "                                \"vital_status\",\n",
    "                                \"days_to_last_followup\",\n",
    "                                \"days_to_death\"]]\n",
    "    \n",
    "    #print(clinicalnew['vital_status'].value_counts())\n",
    "    \n",
    "\n",
    "    # Set missing data to NaN\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Discrepancy]\", 0)\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Not Available]\",np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Applicable]\", np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Available]\", np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Not Available]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Completed]\", np.nan)\n",
    "\n",
    "    # In vital_status set dead = 1 alive = 0\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"Dead\", 1)\n",
    "    clinicalnew[\"vital_status\"] = clinicalnew['vital_status'].replace(\"Alive\", 0)\n",
    "\n",
    "    # Set days to numeric values\n",
    "    clinicalnew[\"days_to_last_followup\"] = pd.to_numeric(clinicalnew[\"days_to_last_followup\"])\n",
    "    clinicalnew[\"days_to_death\"] = pd.to_numeric(clinicalnew[\"days_to_death\"])\n",
    "\n",
    "    # Combine days to death and days to last follow up to create a total time.\n",
    "    clinicalnew['time'] = clinicalnew['days_to_death'].combine_first(clinicalnew['days_to_last_followup'])\n",
    "\n",
    "    # Remove patients that have time 0 (so no follow up, just one recording)\n",
    "    clinicalnew = clinicalnew[clinicalnew.time != 0]\n",
    "\n",
    "    #Remove patients with nan for time or status\n",
    "    clinicalnew = clinicalnew.dropna(subset=['time'])\n",
    "    clinicalnew = clinicalnew.dropna(subset=['vital_status'])\n",
    "    \n",
    "    #Remove patients where time is negative\n",
    "    clinicalnew = clinicalnew[clinicalnew.time >= 0]\n",
    "\n",
    "    mrna.rename(columns={'Unnamed: 0': 'bcr_patient_barcode'}, inplace=True)\n",
    "    mrna = mrna.set_index('bcr_patient_barcode')\n",
    "\n",
    "    mrna = mrna.transpose()\n",
    "    mrna = mrna[mrna.index.isin(clinicalnew['bcr_patient_barcode'])]\n",
    "    mrna = mrna.reindex(np.random.RandomState(seed=1).permutation(mrna.index))\n",
    "    \n",
    "    clinicalnew = clinicalnew.set_index('bcr_patient_barcode')\n",
    "    clinicalnew = clinicalnew.loc[~clinicalnew.index.duplicated(), :]\n",
    "    clinicalnew = clinicalnew.reindex(index=mrna.index)\n",
    "    \n",
    "    x_list.append(mrna)\n",
    "    clinical_processed_list.append(clinicalnew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "586aba9f-d251-48c1-b0a1-60e29372fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the list of processed RNA-seq pandas dataframes together to one large dataset\n",
    "# Drop the NA values\n",
    "# Concatenate the list of processed Clinical pandas dataframes together to one large dataset\n",
    "\n",
    "xf_full = pd.concat(x_list)\n",
    "xf_full = xf_full.dropna(axis=1)\n",
    "clinicalf_full = pd.concat(clinical_processed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc482697-f2cf-4fb7-a224-59efda17c9e1",
   "metadata": {},
   "source": [
    "### Import subset of cancers\n",
    "\n",
    "<p> These cancers are most similar to PDAC, and give a small increase in performance</p>\n",
    "<p> Same procedure as above </p>\n",
    "<ul>\n",
    "    <li>LUAD</li>\n",
    "  <li>STAD</li>\n",
    "  <li>CHOL</li>\n",
    "  <li>SARC</li>\n",
    "  <li>TGCT</li>\n",
    "    <li>COAD</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41151ea3-8c72-4441-969d-efce6ff9c603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/subsetMrna/CHOL.csv\n",
      "Data/subsetMrna/COAD.csv\n",
      "Data/subsetMrna/LUAD.csv\n",
      "Data/subsetMrna/SARC.csv\n",
      "Data/subsetMrna/STAD.csv\n",
      "Data/subsetMrna/TGCT.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a new list mrna_list which contains pandas dataframes of each cancer datasets rna expression data for the subset of cancers\n",
    "# Create a new list clinical_list which contains clinical data for each cancer dataset\n",
    "\n",
    "mrna_list = []\n",
    "mrna_folder = \"Data/subsetMrna\"\n",
    "clinical_folder = \"Data/subsetClinical\"\n",
    "for filename in sorted(os.listdir(mrna_folder)):\n",
    "    f = os.path.join(mrna_folder, filename)\n",
    "    print(f)\n",
    "    data = pd.read_csv(f, sep=\",\")\n",
    "    mrna_list.append(data)\n",
    "    \n",
    "clinical_list = []\n",
    "for filename in sorted(os.listdir(clinical_folder)):\n",
    "    f = os.path.join(clinical_folder, filename)\n",
    "    data = pd.read_csv(f, sep=\"\\t\")\n",
    "    clinical_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756631de-40f0-4efd-b788-30b2eb9e941d",
   "metadata": {},
   "source": [
    "### Prepare subset of cancer data for tensorflow \n",
    "\n",
    "<p> Same steps as described above </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c892cc3-d196-4ee2-95c0-4ef940346bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "x_list = []\n",
    "clinical_processed_list = []\n",
    "\n",
    "for mrna,clinical in zip(mrna_list,clinical_list):\n",
    "    mrna_id = mrna.columns[1:]\n",
    "\n",
    "    # Match clinical samples to genomic samples\n",
    "    clinical.columns = clinical.iloc[0]\n",
    "    clinical = clinical.iloc[2:]\n",
    "\n",
    "    # Make intersection of patient id's that are in mrna and clinicaldata\n",
    "    clinical_id = clinical['bcr_patient_barcode']\n",
    "    intersection = list(set(mrna_id) & set(clinical_id))\n",
    "    intersection.sort()\n",
    "    intersection = pd.Series(intersection)\n",
    "\n",
    "\n",
    "    # Filter out samples in clinicaldata that are not in mrna\n",
    "    a = clinical['bcr_patient_barcode'].isin(intersection)\n",
    "    clinical = clinical[a]\n",
    "\n",
    "\n",
    "\n",
    "    # Create clinicaldata dataframe with the important features\n",
    "    clinicalnew = clinical[['bcr_patient_barcode',\n",
    "                                \"vital_status\",\n",
    "                                \"days_to_last_followup\",\n",
    "                                \"days_to_death\"]]\n",
    "    \n",
    "    #print(clinicalnew['vital_status'].value_counts())\n",
    "    \n",
    "\n",
    "    # Set missing data to NaN\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Discrepancy]\", 0)\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Not Available]\",np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Applicable]\", np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Available]\", np.nan)\n",
    "    clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Not Available]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "    clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Completed]\", np.nan)\n",
    "\n",
    "    # In vital_status set dead = 1 alive = 0\n",
    "    clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"Dead\", 1)\n",
    "    clinicalnew[\"vital_status\"] = clinicalnew['vital_status'].replace(\"Alive\", 0)\n",
    "\n",
    "    # Set days to numeric values\n",
    "    clinicalnew[\"days_to_last_followup\"] = pd.to_numeric(clinicalnew[\"days_to_last_followup\"])\n",
    "    clinicalnew[\"days_to_death\"] = pd.to_numeric(clinicalnew[\"days_to_death\"])\n",
    "\n",
    "    # Combine days to death and days to last follow up to create a total time.\n",
    "    clinicalnew['time'] = clinicalnew['days_to_death'].combine_first(clinicalnew['days_to_last_followup'])\n",
    "\n",
    "    # Remove patients that have time 0 (so no follow up, just one recording)\n",
    "    clinicalnew = clinicalnew[clinicalnew.time != 0]\n",
    "\n",
    "    #Remove patients with nan for time or status\n",
    "    clinicalnew = clinicalnew.dropna(subset=['time'])\n",
    "    clinicalnew = clinicalnew.dropna(subset=['vital_status'])\n",
    "    \n",
    "    #Remove patients where time is negative\n",
    "    clinicalnew = clinicalnew[clinicalnew.time >= 0]\n",
    "\n",
    "    mrna.rename(columns={'Unnamed: 0': 'bcr_patient_barcode'}, inplace=True)\n",
    "    mrna = mrna.set_index('bcr_patient_barcode')\n",
    "\n",
    "    mrna = mrna.transpose()\n",
    "    mrna = mrna[mrna.index.isin(clinicalnew['bcr_patient_barcode'])]\n",
    "    mrna = mrna.reindex(np.random.RandomState(seed=1).permutation(mrna.index))\n",
    "    \n",
    "    clinicalnew = clinicalnew.set_index('bcr_patient_barcode')\n",
    "    clinicalnew = clinicalnew.loc[~clinicalnew.index.duplicated(), :]\n",
    "    clinicalnew = clinicalnew.reindex(index=mrna.index)\n",
    "    \n",
    "    x_list.append(mrna)\n",
    "    clinical_processed_list.append(clinicalnew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93ffa385-1e95-46a8-83c1-ac1747f1dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the list of processed RNA-seq pandas dataframes together to one large dataset\n",
    "# Drop the NA values\n",
    "# Concatenate the list of processed Clinical pandas dataframes together to one large dataset\n",
    "\n",
    "xf_sub = pd.concat(x_list)\n",
    "xf_sub = xf_sub.dropna(axis=1)\n",
    "clinicalf_sub = pd.concat(clinical_processed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c584a92-dfe9-47f8-b942-a68cbc6fe26f",
   "metadata": {},
   "source": [
    "### TCGA PDAC data preparation\n",
    "\n",
    "<p> Same steps as above </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eb2550e7-8a53-4889-94de-f4a697efe9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TCGA PDAC clinical and RNA-seq data\n",
    "\n",
    "clinicaldata = pd.read_csv(f\"Data/TargetDataUnscaled/nationwidechildrens.org_clinical_patient_paad.txt\", sep='\\t')\n",
    "mrna = pd.read_csv(f\"Data/TargetDataUnscaled/PAAD.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "86df2724-f283-4e82-9f06-d45be1f6963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_id = mrna.columns[1:].tolist()\n",
    "\n",
    "# Match clinical samples to genomic samples\n",
    "clinicaldata.columns = clinicaldata.iloc[0]\n",
    "clinicaldata = clinicaldata.iloc[2:]\n",
    "\n",
    "\n",
    "# Make intersection of patient id's that are in mrna and clinicaldata\n",
    "clinical_id = clinicaldata['bcr_patient_barcode']\n",
    "intersection = list(set(mrna_id) & set(clinical_id))\n",
    "intersection.sort()\n",
    "intersection = pd.Series(intersection)\n",
    "\n",
    "\n",
    "# Filter out samples in clinicaldata that are not in mrna\n",
    "a = clinicaldata['bcr_patient_barcode'].isin(intersection)\n",
    "clinicaldata = clinicaldata[a]\n",
    "\n",
    "\n",
    "clinicalnew = clinicaldata\n",
    "\n",
    "\n",
    "# Create clinicaldata dataframe with the important features\n",
    "# clinicalnew = clinicaldata[['bcr_patient_barcode',\n",
    "#                             \"vital_status\",\n",
    "#                             \"days_to_last_followup\",\n",
    "#                             \"days_to_death\"]]\n",
    "\n",
    "\n",
    "\n",
    "# Set missing data to NaN\n",
    "clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"[Discrepancy]\", 0)\n",
    "clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Applicable]\", np.nan)\n",
    "clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Not Available]\", np.nan)\n",
    "clinicalnew[\"days_to_death\"] = clinicalnew['days_to_death'].replace(\"[Discrepancy]\", np.nan)\n",
    "clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Not Available]\", np.nan)\n",
    "clinicalnew['days_to_last_followup'] = clinicalnew['days_to_last_followup'].replace(\"[Discrepancy]\", np.nan)\n",
    "\n",
    "# In vital_status set dead = 1 alive = 0\n",
    "clinicalnew['vital_status'] = clinicalnew['vital_status'].replace(\"Dead\", 1)\n",
    "clinicalnew[\"vital_status\"] = clinicalnew['vital_status'].replace(\"Alive\", 0)\n",
    "\n",
    "# Set days to numeric values\n",
    "clinicalnew[\"days_to_last_followup\"] = pd.to_numeric(clinicalnew[\"days_to_last_followup\"])\n",
    "clinicalnew[\"days_to_death\"] = pd.to_numeric(clinicalnew[\"days_to_death\"])\n",
    "\n",
    "# Combine days to death and days to last follow up to create a total time.\n",
    "clinicalnew['time'] = clinicalnew['days_to_death'].combine_first(clinicalnew['days_to_last_followup'])\n",
    "\n",
    "# Remove patients that have time 0 (so no follow up, just one recording)\n",
    "clinicalnew = clinicalnew[clinicalnew.time != 0]\n",
    "\n",
    "#Remove patients with nan for time\n",
    "clinicalnew = clinicalnew.dropna(subset=['time'])\n",
    "\n",
    "mrna.rename(columns={'Unnamed: 0': 'bcr_patient_barcode'}, inplace=True)\n",
    "mrna = mrna.set_index('bcr_patient_barcode')\n",
    "\n",
    "mrna = mrna.transpose()\n",
    "mrna = mrna[mrna.index.isin(clinicalnew['bcr_patient_barcode'])]\n",
    "mrna = mrna.reindex(np.random.RandomState(seed=1).permutation(mrna.index))\n",
    "clinicalnew = clinicalnew.set_index('bcr_patient_barcode')\n",
    "clinicalnew = clinicalnew.reindex(index=mrna.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0589a1b4-1994-4304-aa31-5cd373de3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = mrna\n",
    "yt = clinicalnew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ac774-d47e-48ec-b6b4-161da7854dac",
   "metadata": {},
   "source": [
    "### TCGA ICGC Data\n",
    "<p> Similar steps to above </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25d1a318-3722-4d20-b3bc-a84e2de226e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ICGC clinical and mrna data\n",
    "\n",
    "clinicaldata = pd.read_csv(f\"Data/ICGCDataUnscaled/PDAC_ICGC_clinical.csv\", sep=',')\n",
    "mrna = pd.read_csv(f\"Data/ICGCDataUnscaled/PDAC_ICGC.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1c5fd80-b6a5-4308-826b-1264fd88a923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == \"__main__\":\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == \"\":\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Create clinicaldata dataframe with the important features\n",
    "clinicalnew = clinicaldata[['icgc_donor_id',\n",
    "                            \"donor_vital_status\",\n",
    "                            \"donor_survival_time\"]]\n",
    "\n",
    "\n",
    "\n",
    "# Set missing data to NaN\n",
    "clinicalnew['donor_vital_status'] = clinicalnew['donor_vital_status'].replace(\"\", np.nan)\n",
    "\n",
    "# In vital_status set dead = 1 alive = 0\n",
    "clinicalnew['donor_vital_status'] = clinicalnew['donor_vital_status'].replace(\"deceased\", 1)\n",
    "clinicalnew[\"donor_vital_status\"] = clinicalnew['donor_vital_status'].replace(\"alive\", 0)\n",
    "\n",
    "# Set days to numeric values\n",
    "clinicalnew[\"donor_survival_time\"] = pd.to_numeric(clinicalnew[\"donor_survival_time\"])\n",
    "\n",
    "# Combine days to death and days to last follow up to create a total time.\n",
    "clinicalnew['time'] = clinicalnew['donor_survival_time']\n",
    "\n",
    "# Remove patients that have time 0 (so no follow up, just one recording)\n",
    "clinicalnew = clinicalnew[clinicalnew.time != 0]\n",
    "\n",
    "#Remove patients with nan for time\n",
    "clinicalnew = clinicalnew.dropna(subset=['time'])\n",
    "\n",
    "mrna.rename(columns={'Unnamed: 0': 'icgc_donor_id'}, inplace=True)\n",
    "mrna = mrna.set_index('icgc_donor_id')\n",
    "\n",
    "clinicalnew = clinicalnew.set_index('icgc_donor_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3885bba0-4760-4116-83eb-07b6ded19e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "xv = mrna\n",
    "yv = clinicalnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "015acc1a-002e-4c39-9c37-a92858657920",
   "metadata": {},
   "outputs": [],
   "source": [
    "xv = xv.drop(['DO49201'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0ba05-6965-4e14-84b3-6d513407bc67",
   "metadata": {},
   "source": [
    "### Match the genes in all 3 datasets to each other\n",
    "<p> We now have 3 datasets: A large multi-cancer dataset, a PDAC dataset, and a ICGC PDAC dataset. </p>\n",
    "<p>Here we make sure all datasets contain the same genes, and the order of the genes is the same </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c255f8b-7773-4357-956c-ea3a9fe74c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the column names into transcript id's and genes again to make similar to ICGC data later on\n",
    "\n",
    "columns = xt.columns.str.split(' / ')\n",
    "correct_columns = []\n",
    "for column in columns:\n",
    "    correct_columns.append(column[0])\n",
    "\n",
    "xt.columns = correct_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e69245fa-e247-4fd0-8a35-878bf2d0f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the column names into transcript id's and genes again to make similar to ICGC data later on\n",
    "\n",
    "columns = xf_full.columns.str.split(' / ')\n",
    "correct_columns = []\n",
    "for column in columns:\n",
    "    correct_columns.append(column[0])\n",
    "\n",
    "xf_full.columns = correct_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3010a4a8-7ee9-4afd-9213-f6de4d3b1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the column names into transcript id's and genes again to make similar to ICGC data later on\n",
    "\n",
    "columns = xf_sub.columns.str.split(' / ')\n",
    "correct_columns = []\n",
    "for column in columns:\n",
    "    correct_columns.append(column[0])\n",
    "\n",
    "xf_sub.columns = correct_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b02dccf-6e0d-40b9-904e-eb95114d2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the genes in the TCGA PDAC data that are not in the full 32 cancer dataset\n",
    "\n",
    "labels_to_drop = xt.columns.difference(xf_full.columns)\n",
    "xt = xt.drop(labels=labels_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59925601-a7db-4a02-8a9a-dfa7e3789cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all duplicated genes in the three dataset\n",
    "\n",
    "xt = xt.loc[:,~xt.columns.duplicated()].copy()\n",
    "xf_full = xf_full.loc[:,~xf_full.columns.duplicated()].copy()\n",
    "xf_sub = xf_sub.loc[:,~xf_sub.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69e9e7b1-6091-4c42-b86a-8f66c391ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop genes from datasets so that they are all similar in number\n",
    "\n",
    "xt = xt.drop(labels=(xt.columns.difference(xf_full.columns)),axis=1)\n",
    "xv = xv.drop(labels=(xv.columns.difference(xf_full.columns)),axis=1)\n",
    "xf_full = xf_full.drop(labels=(xf_full.columns.difference(xt.columns)),axis=1)\n",
    "xf_sub = xf_sub.drop(labels=(xf_sub.columns.difference(xt.columns)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a4e7829-a6fa-44d7-a87c-ea15a23f2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop genes from TCGA data so they are similar to ICGC data\n",
    "\n",
    "xt = xt.drop(labels=(xt.columns.difference(xv.columns)),axis=1)\n",
    "xf_full = xf_full.drop(labels=(xf_full.columns.difference(xv.columns)),axis=1)\n",
    "xf_sub = xf_sub.drop(labels=(xf_sub.columns.difference(xv.columns)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5bacb-551a-4d55-a684-ae225f063dc2",
   "metadata": {},
   "source": [
    "Check if number of features/genes is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab6fee5c-2ea4-4b0d-aa64-f4708742a204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 12668)\n",
      "(8685, 12668)\n",
      "(1529, 12668)\n",
      "(72, 12668)\n",
      "(8828, 12668)\n"
     ]
    }
   ],
   "source": [
    "print(xt.shape)\n",
    "print(xf_full.shape)\n",
    "print(xf_sub.shape)\n",
    "print(xv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db3b0f73-1487-4724-b88f-210c81a28ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the order of the genes to how to they occur in the TCGA PDAC data\n",
    "\n",
    "xv = xv[xt.columns]\n",
    "xf_full = xf_full[xt.columns]\n",
    "xf_sub = xf_sub[xt.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800cd7d-0b02-4298-8142-5ed8c4c98bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Source model\n",
    "<p> Start with setting pandas dataframes to numpy arrays </p>\n",
    "<p> Choose the first code block underneath to use the 32 cancer dataset as the source dataset </p>\n",
    "<p> Choose the second code block underneath to use the subset of cancers similar to PDAC </p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b4518466-0894-484c-9c4d-017b8902c96f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set pandas dataframes to numpy arrays, also create numpy arrays from the time and vital status data\n",
    "# The large 32 cancer dataset is used here\n",
    "\n",
    "xf = xf_full_paad.to_numpy()\n",
    "yf = clinicalf_full_paad\n",
    "\n",
    "ytime = yf['time'].to_numpy()\n",
    "ystatus = yf['vital_status'].to_numpy()\n",
    "ystatusbool = yf['vital_status'].astype(np.bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dc93a416-30ab-444d-b524-cb26f80b2e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set pandas dataframes to numpy arrays, also create numpy arrays from the time and vital status data\n",
    "# The subset of cancers similar to PDAC dataset is used here\n",
    "\n",
    "xf = xf_sub.to_numpy()\n",
    "yf = clinicalf_sub\n",
    "\n",
    "ytime = yf['time'].to_numpy()\n",
    "ystatus = yf['vital_status'].to_numpy()\n",
    "ystatusbool = yf['vital_status'].astype(np.bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2898fb-196b-4c98-962a-92f088d7d1ef",
   "metadata": {},
   "source": [
    "<p> Here we set our quantiles and make new labels that assign a quantile to each patient. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16ad9037-5460-4d78-a45c-b6fb1bda97e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create time quantiles to later be used in stratification\n",
    "\n",
    "first_quantile = np.quantile(ytime,0.25)\n",
    "second_quantile = np.quantile(ytime,0.5)\n",
    "third_quantile = np.quantile(ytime,0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d4f2895-c3b7-4520-9ee6-66449fcc5807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every patient's time value in ytime check in what quantile it falls and make a new list that contains the corresponding time quantile for each patient\n",
    "\n",
    "yquantile = []\n",
    "for time in ytime:\n",
    "    if time >= 0 and time < first_quantile:\n",
    "        yquantile.append(1)\n",
    "    elif time >= first_quantile and time < second_quantile:\n",
    "        yquantile.append(2)\n",
    "    elif time >= second_quantile and time < third_quantile:\n",
    "        yquantile.append(3)\n",
    "    elif time >= third_quantile:\n",
    "        yquantile.append(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6151ec8-bdc1-4d0e-93d2-d89932a01fb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set time interval for nnet survival custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200047eb-5180-4259-b8f1-1b2bf92c5cde",
   "metadata": {
    "tags": []
   },
   "source": [
    "><p>The time intervals should be set so that for every interval an equal amount of events occur. This is done with matplotlib and manually adjusting the bins untill you see an equal number of events per interval</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13e476-75e0-41bb-b2ac-2eaa95874bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many death events are in the data so you know how many death events you need per time interval\n",
    "\n",
    "number_of_events = (yf.vital_status == 1).sum()\n",
    "number_of_nonevents = (yf.vital_status == 0).sum()\n",
    "\n",
    "print(number_of_events)\n",
    "print(number_of_nonevents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eac9e753-04b4-48b8-8ec8-79c882e5f05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([17., 17., 16., 17., 16., 17., 15., 15., 17., 16., 17., 17., 16.,\n",
       "        16., 16., 17., 18., 18., 17., 17.]),\n",
       " array([   0,   40,   98,  153,  180,  240,  280,  330,  370,  428,  495,\n",
       "         580,  660,  750,  860,  990, 1170, 1400, 1660, 2100, 7000]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a plot where you set the time interval bins in the bins variable, the plot should be as straight as possible, containing an equal number of death events per time interval\n",
    "\n",
    "test = yf.loc[yf['vital_status'] == 1]\n",
    "\n",
    "plt.hist(test['time'], bins=[0,55,100,135,180,240,260,322,370,430,455,520,580,620,730,840,970,1140,1350,1590,1900,2540,9200],ec='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e91a5-74b3-46e4-9797-dc38bc08ef0e",
   "metadata": {},
   "source": [
    "><p> Here we set our time intervals and create the survival matrix used by the nnet-survival model </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8f8bf42e-6f86-4c60-b437-fc1bb0ee0b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we set the breaks for our time intervals which we obtained with the plot above\n",
    "# Use the first breaks for the large 32 cancer dataset, use the second breaks for the subset of cancers\n",
    "\n",
    "breaks=np.asarray([0,55,100,135,180,240,260,322,370,430,455,520,580,620,730,840,970,1140,1350,1590,1900,2540,9200])\n",
    "# breaks=np.asarray([0,40,98,153,180,240,280,330,370,428,495,580,660,750,860,990,1170,1400,1660,2100,7000])\n",
    "n_intervals = len(breaks)-1\n",
    "timegap = breaks[1:] - breaks[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea30958b-ff3a-4676-8bf4-fb887f8d386d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the survival array using the nnet_survival fuction 'make_surv_array'\n",
    "# This function takes the following input:\n",
    "#    ytime - numpy array of each patient's time until death/censor value\n",
    "#    ystatus - numpy array of each patient's survival status (1 for death event, 0 for alive/censored)\n",
    "#    breaks - the breaks for the time intervals that were \n",
    "\n",
    "yf = nnet_survival.make_surv_array(ytime, ystatus, breaks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca6cb1-5358-4e6c-9e54-9fee5f986722",
   "metadata": {},
   "source": [
    "<p> This step adds the status and quantile labels to our survival matrix <br>\n",
    "This is used only with our 5-fold CV hyperparameter optimization</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f49ad0c5-2f30-40d2-9a6f-3db531919ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every survival matrix, status value, and quantile value merge the status and quantile values together and merge with the survival matrix.\n",
    "\n",
    "yf_final = ([],[])\n",
    "\n",
    "for matrix,status,quantile in zip(yf,ystatus,yquantile):\n",
    "    yf_final[0].append(matrix)\n",
    "    status_quantile = (str(int(status)) + \"_\" + str(quantile))\n",
    "    yf_final[1].append(status_quantile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15fec28b-6cb9-4601-8624-af0aa08b73aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2014c878-582f-4139-8d08-cd38b4cdbe7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Custom class for hypermodel + concordance index function\n",
    "<p> This code block shows 2 custom classes, one being a hypermodel that is built by our hyperparameter tuner, and where you can add variable parameters which will be tested. <br>\n",
    "The other is a custom hyper parameter tuner that utilizes cross validation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cfd29e6a-4b2a-4bee-959c-715f0ff251ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SurvivalHyperModel(HyperModel):\n",
    "    def __init__(self, n_intervals, weights=[]):\n",
    "        self.n_intervals = n_intervals\n",
    "        self.weights = weights\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        hp_l2_value = hp.Choice('l2_value', values=[0.01,0.001,0.0001,0.00001])\n",
    "        model.add(Dense(np.sqrt(xf.shape[1]), input_dim=xf.shape[1], bias_initializer='zeros',activation='relu', kernel_regularizer=regularizers.l2(hp_l2_value)))\n",
    "        model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "        model.add(nnet_survival.PropHazards(self.n_intervals))\n",
    "        model.compile(loss=nnet_survival.surv_likelihood(self.n_intervals), optimizer=optimizers.Adam(learning_rate=0.00001),run_eagerly=False)\n",
    "        if self.weights:\n",
    "            model.layers[0].set_weights(self.weights)\n",
    "            return model\n",
    "        else:\n",
    "            return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,\n",
    "                         shuffle=False,\n",
    "                         **kwargs,\n",
    "                        )\n",
    "    \n",
    "class CVTuner(keras_tuner.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, x, y,epochs=1, *args, **kwargs):\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "        skf.get_n_splits(x,y[1])\n",
    "        val_losses = []\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Choice('batch_size', values=[8,16,32])\n",
    "        fold = 1\n",
    "        hp = trial.hyperparameters\n",
    "        for train_indices, test_indices in skf.split(x,y[1]):\n",
    "            early_stopping_hp = EarlyStopping(monitor='val_loss', patience=(100),min_delta=0.0005, restore_best_weights=True)\n",
    "            dir_for_logs = (\"Logs/ktuner/logs_pdac_21/\" + str(trial.trial_id) + \"/fold_\" + str(fold))\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=dir_for_logs, histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "            callbacks_list = [early_stopping_hp,tensorboard_callback]\n",
    "            x_train, x_test = x[train_indices], x[test_indices]\n",
    "            y_train, y_test = np.array(y[0])[train_indices], np.array(y[0])[test_indices]\n",
    "            y_train_labels, y_test_labels = np.array(y[1])[train_indices], np.array(y[1])[test_indices]\n",
    "            model = self.hypermodel.build(hp)\n",
    "            model.fit(x_train, y_train,validation_data=(x_test,y_test), batch_size=kwargs['batch_size'], epochs=epochs, callbacks=callbacks_list, verbose=0)       \n",
    "            y_pred = model.predict(x_test, verbose=0)\n",
    "            surv_prob=np.cumprod(y_pred, axis=1)[:,-1]\n",
    "            val_losses.append(model.evaluate(x_test, y_test))\n",
    "            fold += 1\n",
    "        self.oracle.update_trial(trial.trial_id, {'val_loss': np.mean(val_losses)})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0fec9e-0dc2-4273-ae32-dcdac067294b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameter optimization and training of the multi-cancer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c1d1a-4d64-4604-975d-d1e0baf434b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p> In this section of code the hyperparameter tuning of the multi-cancer model occurs. <br>\n",
    "    The Hyperparameters are tuned on the whole datasets, and the results can be viewed with tensorboard. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b89c1-b01c-47b3-a802-570278fe83a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xf_t = scaler.fit_transform(xf)\n",
    "\n",
    "# logfile - Filename in which the results will be stored of the hyperparameter tuning.\n",
    "#           Call this file with Tensorboard to view loss curves and hyperparameter tuning results.\n",
    "#\n",
    "# name_of_project - Name of the Keras Tuner project.\n",
    "#                   Make sure this is different if you want to try a new search, else it will \n",
    "#                   just give you the results of the project previous project that had the same name !!!!\n",
    "#\n",
    "# name_of_dir - Directory where projects will be stored.\n",
    "\n",
    "logfile = \"Logs/ktuner/logs_pretrain_9_5/\"\n",
    "name_of_project = \"pretrain_model_9_5\"\n",
    "name_of_dir = \"pretrain_model_dir\"\n",
    "\n",
    "# The Cross validation tuner class, takes as input:\n",
    "#    hypermodel - A hypermodel class, which is your model but contains hyperparameter values for every parameter you want to test\n",
    "#    directory - Directory where the hyper parameter tuning project will be stored\n",
    "#    logger - Takes a TensorBoardLogger value as input which will store the loss of the model during training steps, \n",
    "#             which can be used in TensorBoard to create plots\n",
    "#    oracle - The keras tuner optimization method that you want to use, refer to: https://keras.io/api/keras_tuner/oracles/ for possible oracles.\n",
    "#             takes as input:\n",
    "#             objective - The objective of the hyperparameter tuning, and what determines whether a set of parameters is 'good'\n",
    "#             max_trials - The number of trials you wish to perform, each trial will contain a set of parameters to test.\n",
    "#\n",
    "# After creating the class, use .search to initiate the hyperparameter tuning, takes as input:\n",
    "#    xf_t - the scaled input features to train on\n",
    "#    yf_final - the label on which predictions will be assessed, contains the survival matrix and status_quantile for a patient\n",
    "#    epochs - the maximum number of epochs you wish to train (Early stopping is included in the class, so value is determined automatically)\n",
    "#    verbose - 1 to see progress of hyperparameter tuning in console, 0 to not see progress.\n",
    "                                                \n",
    "tuner = CVTuner(\n",
    "    hypermodel=SurvivalHyperModel(n_intervals),\n",
    "    project_name=name_of_project,\n",
    "    directory=name_of_dir,\n",
    "    logger = TensorBoardLogger(metrics=[\"val_loss\"], logdir=logfile),\n",
    "    oracle=kt.oracles.BayesianOptimization(\n",
    "        objective='val_loss',\n",
    "        max_trials=10))\n",
    "tuner.search(xf_t,yf_final,epochs=40000,verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0c26e-8a6a-41ea-a5cc-c4e1bd3b6265",
   "metadata": {},
   "source": [
    "<p> Now the source model is ran using the optimal parameters found in the previous step </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e88425bf-8b48-4016-b40b-097f3937d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 10:00:50.520361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:50.626508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:50.627256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:50.630545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 10:00:50.632252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:50.632944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:50.633625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:52.466143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:52.466861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:52.467469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 10:00:52.468094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13823 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_index: 0.9296680882252264\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "# Set the optimal parameters found for the source model\n",
    "\n",
    "optimal_l2 = 0.01\n",
    "optimal_batch = 16\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xf_t = scaler.fit_transform(xf)\n",
    "\n",
    "# Set the labels to the survival matrices\n",
    "\n",
    "yf = np.array(yf_final[0])\n",
    "\n",
    "# Create the model with the optimal parameters\n",
    "\n",
    "optimal_model = Sequential()\n",
    "optimal_model.add(Dense(np.sqrt(xf.shape[1]), input_dim=xf.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "optimal_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "optimal_model.add(nnet_survival.PropHazards(n_intervals))\n",
    "opt = optimizers.Adam(learning_rate=0.00001)\n",
    "optimal_model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=opt)\n",
    "\n",
    "\n",
    "# Create a tensorboard callback which saves the loss during the training of the model\n",
    "\n",
    "dir_for_log = (\"Logs/ktuner/source_model_full_paad\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=dir_for_log, histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "# Fit the model to the input data xf_t and yf\n",
    "\n",
    "history = optimal_model.fit(xf_t,yf,batch_size=optimal_batch,epochs=1200, callbacks=callbacks_list,verbose=0)\n",
    "\n",
    "# Save the weights of the source model\n",
    "\n",
    "optimal_model.save_weights(\"Weights/source_model_full_paad\")   \n",
    "\n",
    "# Get performance of source model on full dataset which is very optimistic\n",
    "\n",
    "print(\"Performance model on full dataset (very optimistic):\")\n",
    "y_pred = optimal_model.predict(xf_t, verbose=0)\n",
    "oneyr_surv = np.cumprod(y_pred[:, 0:np.nonzero(breaks > 365)[0][0]], axis=1)[:, -1]\n",
    "c_index_test = concordance_index(ytime, oneyr_surv, ystatus)\n",
    "print(\"C_index: \" + str(c_index_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74a5e0-6b2e-49ae-b048-34a67921cf4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Data preperation TCGA PDAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c66b21b5-769c-464d-850a-bba1fca4d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas dataframes of TCGA RNA-seq data to numpy array\n",
    "# Set pandas dataframes of TCGA clinical data (time and vital_status) to numpy arrays\n",
    "\n",
    "xt = xt.to_numpy()\n",
    "#\n",
    "ytime = yt['time'].to_numpy()\n",
    "ystatus = yt['vital_status'].to_numpy()\n",
    "ystatusbool = yt['vital_status'].astype(np.bool).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "806428f3-86aa-48d8-a9de-0b8728d909fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15., 14., 15., 15., 14., 13., 14., 14., 14., 15.]),\n",
       " array([   0,   17,   70,  130,  180,  230,  300,  400,  580,  730, 2200]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a plot where you set the time interval bins in the bins variable, the plot should be as straight as possible, containing an equal number of death events per time interval\n",
    "\n",
    "test = yt.loc[yt['vital_status'] == 1]\n",
    "\n",
    "plt.hist(yt['time'], bins=[0,17,70,130,180,230,300,400,580,730,2200],ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "10a6ee66-d274-4cb9-9ab0-1db528e4a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the breaks for our time intervals which we obtained with the plot above\n",
    "\n",
    "breaks=np.array([0,110,130,190,250,310,475,550,650,730,2200])\n",
    "n_intervals = len(breaks) - 1\n",
    "timegap = breaks[1:] - breaks[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ebd2499f-79f5-4138-b47f-1aa15447eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the survival array for the TCGA PDAC patients using the nnet_survival fuction 'make_surv_array'\n",
    "# This function takes the following input:\n",
    "#    ytime - numpy array of each patient's time until death/censor value\n",
    "#    ystatus - numpy array of each patient's survival status (1 for death event, 0 for alive/censored)\n",
    "#    breaks - the breaks for the time intervals that were \n",
    "\n",
    "y_t = nnet_survival.make_surv_array(ytime, ystatus, breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8960c527-70b4-4b76-9049-d9db7163899b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.0\n",
      "228.0\n",
      "482.5\n"
     ]
    }
   ],
   "source": [
    "# Create time quantiles to later be used in stratification\n",
    "\n",
    "first_quantile = np.quantile(ytime,0.25)\n",
    "second_quantile = np.quantile(ytime,0.5)\n",
    "third_quantile = np.quantile(ytime,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6df28d9c-9183-47d0-9165-52463e52e46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every patient's time value in ytime check in what quantile it falls and make a new list that contains the corresponding time quantile for each patient\n",
    "\n",
    "yquantile = []\n",
    "for time in ytime:\n",
    "    if time >= 0 and time < first_quantile:\n",
    "        yquantile.append(1)\n",
    "    elif time >= first_quantile and time < second_quantile:\n",
    "        yquantile.append(2)\n",
    "    elif time >= second_quantile and time < third_quantile:\n",
    "        yquantile.append(3)\n",
    "    elif time >= third_quantile:\n",
    "        yquantile.append(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "55b13e65-b4a9-4c6b-83ba-b277798617af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every survival matrix, status value, and quantile value merge the status and quantile values together and merge with the survival matrix.\n",
    "\n",
    "yt_final = ([],[])\n",
    "\n",
    "for matrix,status,quantile in zip(y_t,ystatus,yquantile):\n",
    "    yt_final[0].append(matrix)\n",
    "    status_quantile = (str(int(status)) + \"_\" + str(quantile))\n",
    "    yt_final[1].append(status_quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e3dbe-e3c7-4527-a708-1b20467eac49",
   "metadata": {},
   "source": [
    "### Data preperation ICGC \n",
    "<p> Same steps as TCGA data, again make sure there are similar amounts of death events per time interval. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0b085-5737-4a38-aff8-e70ea1ac1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas dataframes of ICGC RNA-seq data to numpy array\n",
    "# Set pandas dataframes of ICGC clinical data (time and vital_status) to numpy arrays\n",
    "\n",
    "xv = xv.to_numpy()\n",
    "\n",
    "yvtime = yv['time'].to_numpy()\n",
    "yvstatus = yv['donor_vital_status'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "12d37fd8-ce54-4449-9513-290f9e647a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array which contains the status and survival in days for each patient, this is used by the Cox-ph model as the labels\n",
    "\n",
    "ystatustime = []\n",
    "for status,time in zip(yvstatus,yvtime):\n",
    "    ystatustime.append((status,time))\n",
    "\n",
    "ysurvival_data = np.array(ystatustime, dtype=[('Status', '?'), ('Survival_in_days', '<f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c80ad422-9661-4299-a6d7-527f599604f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5., 5., 4., 5., 4., 4., 4., 5., 5., 5.]),\n",
       " array([   0,  165,  230,  260,  345,  390,  425,  460,  600, 1100, 1880]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a plot where you set the time interval bins in the bins variable, the plot should be as straight as possible, containing an equal number of death events per time interval\n",
    "\n",
    "test = yv.loc[yv['donor_vital_status'] == 1]\n",
    "\n",
    "plt.hist(test['time'], bins=[0,165,230,260,345,390,425,460,600,1100,1880],ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b1372454-6e2c-4fc1-a8c7-73b18abe5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the breaks for our time intervals which we obtained with the plot above\n",
    "\n",
    "vbreaks = np.array([0,165,230,260,345,390,425,460,600,1100,1880])\n",
    "n_intervals = len(vbreaks) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1929b9dd-10f2-40be-a043-ddf8a0675646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the survival array for the ICGC PDAC patients using the nnet_survival fuction 'make_surv_array'\n",
    "# This function takes the following input:\n",
    "#    ytime - numpy array of each patient's time until death/censor value\n",
    "#    ystatus - numpy array of each patient's survival status (1 for death event, 0 for alive/censored)\n",
    "#    breaks - the breaks for the time intervals that were \n",
    "\n",
    "y_v = nnet_survival.make_surv_array(yvtime, yvstatus, vbreaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3998e33f-a14c-4319-90d6-4d3ef9421263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.5\n",
      "413.5\n",
      "719.5\n"
     ]
    }
   ],
   "source": [
    "# Create time quantiles to later be used in stratification\n",
    "\n",
    "first_quantile_v = np.quantile(yvtime,0.25)\n",
    "second_quantile_v = np.quantile(yvtime,0.5)\n",
    "third_quantile_v = np.quantile(yvtime,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "426f383e-a1a6-4999-bac9-2091a64c70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every patient's time value in yvtime check in what quantile it falls and make a new list that contains the corresponding time quantile for each patient\n",
    "\n",
    "yvquantile = []\n",
    "for time in yvtime:\n",
    "    if time >= 0 and time < first_quantile_v:\n",
    "        yvquantile.append(1)\n",
    "    elif time >= first_quantile_v and time < second_quantile_v:\n",
    "        yvquantile.append(2)\n",
    "    elif time >= second_quantile_v and time < third_quantile_v:\n",
    "        yvquantile.append(3)\n",
    "    elif time >= third_quantile_v:\n",
    "        yvquantile.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "23b17c52-a269-43c0-8b4f-0ed7a7e46471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every survival matrix, status value, and quantile value merge the status and quantile values together and merge with the survival matrix.\n",
    "\n",
    "yv_final = ([],[])\n",
    "\n",
    "for matrix,status,quantile in zip(y_v,yvstatus,yvquantile):\n",
    "    yv_final[0].append(matrix)\n",
    "    status_quantile = (str(int(status)) + \"_\" + str(quantile))\n",
    "    yv_final[1].append(status_quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1058a4-f714-4270-a0a2-16e69f42e959",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameter optimization and training of the TCGA PDAC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01262c13-0f0c-4266-836b-15aac375e605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xt = scaler.fit_transform(xt)\n",
    "\n",
    "# logfile - Filename in which the results will be stored of the hyperparameter tuning.\n",
    "#           Call this file with Tensorboard to view loss curves and hyperparameter tuning results.\n",
    "#\n",
    "# name_of_project - Name of the Keras Tuner project.\n",
    "#                   Make sure this is different if you want to try a new search, else it will \n",
    "#                   just give you the results of the project previous project that had the same name !!!!\n",
    "#\n",
    "# name_of_dir - Directory where projects will be stored.\n",
    "\n",
    "logfile = \"Logs/ktuner/logs_tcga_model/\"\n",
    "name_of_project = \"tcga_model\"\n",
    "name_of_dir = \"tcga_model_dir\"\n",
    "\n",
    "# The Cross validation tuner class, takes as input:\n",
    "#    hypermodel - A hypermodel class, which is your model but contains hyperparameter values for every parameter you want to test\n",
    "#    directory - Directory where the hyper parameter tuning project will be stored\n",
    "#    logger - Takes a TensorBoardLogger value as input which will store the loss of the model during training steps, \n",
    "#             which can be used in TensorBoard to create plots\n",
    "#    oracle - The keras tuner optimization method that you want to use, refer to: https://keras.io/api/keras_tuner/oracles/ for possible oracles.\n",
    "#             takes as input:\n",
    "#             objective - The objective of the hyperparameter tuning, and what determines whether a set of parameters is 'good'\n",
    "#             max_trials - The number of trials you wish to perform, each trial will contain a set of parameters to test.\n",
    "#\n",
    "# After creating the class, use .search to initiate the hyperparameter tuning, takes as input:\n",
    "#    xf_t - the scaled input features to train on\n",
    "#    yf_final - the label on which predictions will be assessed, contains the survival matrix and status_quantile for a patient\n",
    "#    epochs - the maximum number of epochs you wish to train (Early stopping is included in the class, so value is determined automatically)\n",
    "#    verbose - 1 to see progress of hyperparameter tuning in console, 0 to not see progress.\n",
    "                                                \n",
    "tuner = CVTuner(\n",
    "    hypermodel=SurvivalHyperModel(n_intervals),\n",
    "    project_name=name_of_project,\n",
    "    directory=name_of_dir,\n",
    "    logger = TensorBoardLogger(metrics=[\"val_loss\"], logdir=logfile),\n",
    "    oracle=kt.oracles.BayesianOptimization(\n",
    "        objective='val_loss',\n",
    "        max_trials=10))\n",
    "tuner.search(xt,yt_final,epochs=40000,verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15ca77-6487-4ac4-90b7-a0f484b31f17",
   "metadata": {},
   "source": [
    "### Run TCGA PDAC model just once and store weights (Setting 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9109b96-8a04-4032-900a-560efb0d3288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12668\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "xt_t = scaler.fit_transform(xt)\n",
    "\n",
    "# Set the optimal parameters found for the model\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "# Create the model with the optimal parameters\n",
    "model = Sequential()\n",
    "model.add(Dense(np.sqrt(xt.shape[1]), input_dim=xt.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "model.add(nnet_survival.PropHazards(n_intervals))\n",
    "model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Create a tensorboard callback which saves the loss during the training of the model\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_finetune_model\"), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "# Fit the model to the input data xt_t and y_t\n",
    "history = model.fit(xt_t,y_t,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0) \n",
    "\n",
    "# Save the weights of the model\n",
    "model.save_weights(\"Weights/finetune_model\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22776cc-fdb7-400e-b93b-3665145365a8",
   "metadata": {},
   "source": [
    "### Run TCGA PDAC Model with transferred weights from Multi-Cancer Model (Fine-tuning step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "770336c0-57d8-4404-960f-91ecc76eb9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "# Scale the input features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "xt_t = scaler.fit_transform(xt)\n",
    "\n",
    "# Set the optimal parameters found for the model\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "# Create a model to initialize the weights of the source model into (should have the same architecture)\n",
    "init_model = Sequential()\n",
    "init_model.add(Dense(np.sqrt(xt.shape[1]), input_dim=xt.shape[1], bias_initializer='zeros', activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "init_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "init_model.add(nnet_survival.PropHazards(22))\n",
    "init_model.compile(loss=nnet_survival.surv_likelihood(22), optimizer=optimizers.Adam(learning_rate=0.00001))\n",
    "\n",
    "# Load the weights of the source model into the initialization model\n",
    "init_model.load_weights(\"Weights/source_model_subset\")\n",
    "\n",
    "\n",
    "# Create a model to run the fine tuning step on (this uses the source model's weights)\n",
    "model = Sequential()\n",
    "model.add(Dense(np.sqrt(xt.shape[1]), input_dim=xt.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "model.add(nnet_survival.PropHazards(n_intervals))\n",
    "model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Transfer weights of source model's second hidden layer to fine tuning model's second hidden layer\n",
    "model.layers[1].set_weights(init_model.layers[1].get_weights())\n",
    "\n",
    "\n",
    "# Create a tensorboard callback which saves the loss during the training of the model\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_source_finetune_model_subset\"), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "# Fit the model to the input data xt_t and y_t\n",
    "history = model.fit(xt_t,y_t,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0) \n",
    "\n",
    "# Save the weights of the model\n",
    "model.save_weights(\"Weights/source_finetune_model_subset\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357fc7b-ab3e-4e18-a99a-3ac0fea2a3c4",
   "metadata": {},
   "source": [
    "### Train ICGC PDAC and validate with 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3210d7c-00b8-4802-ba9d-1bf78d2a9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nnet_survival)\n",
    "\n",
    "# Set transfer_learning to True, if you want transfer learning, else set it too False\n",
    "transfer_learning = True\n",
    "\n",
    "# File which will store the C-indexes obtained\n",
    "results_file = \"Results/cindexes_fullsetpaad\"\n",
    "\n",
    "cindex_list = []\n",
    "cindex_train_list = []\n",
    "\n",
    "total_cindexes = []\n",
    "total_cindexes_train = []\n",
    "\n",
    "# Set optimal parameters found with hyperparameter tuning\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "\n",
    "if transfer_learning == True:\n",
    "    # Create a model to initialize the weights of the source model into (should have the same architecture)\n",
    "    \n",
    "    init_model = Sequential()\n",
    "    init_model.add(Dense(np.sqrt(xt.shape[1]), input_dim=xt.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    init_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "    init_model.add(nnet_survival.PropHazards(22))\n",
    "    init_model.compile(loss=nnet_survival.surv_likelihood(10), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "    # Load the weights of the source model into the initialization model\n",
    "\n",
    "    init_model.load_weights(\"Weights/source_model_full_paad\")\n",
    "\n",
    "# Create Repeated Stratified Kfold cross validation object\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=24)\n",
    "\n",
    "k = 1\n",
    "\n",
    "test_performance = []\n",
    "train_performance = []\n",
    "\n",
    "\n",
    "# Start the repeated 5 fold cross validation (will repeat 50 times, based on n_splits = 5 and n_repeats = 10)\n",
    "for train_indices, test_indices in rskf.split(xv,yv_final[1]):\n",
    "    print(\"fold: \" + str(k) + \"/50\")\n",
    "    \n",
    "    # Create splits of the input data\n",
    "    # Split the input features of the ICGC PDAC data\n",
    "    x_train, x_test = xv[train_indices], xv[test_indices]\n",
    "    # Split the input labels of the ICGC PDAC data (survival matrices)\n",
    "    y_train, y_test = np.array(yv_final[0])[train_indices], np.array(yv_final[0])[test_indices]\n",
    "    # Split the status_quantile variables for each patient\n",
    "    y_train_labels, y_test_labels = np.array(yv_final[1])[train_indices], np.array(yv_final[1])[test_indices]\n",
    "    # Split the status values for each patient\n",
    "    ystatus_train, ystatus_test = yvstatus[train_indices], yvstatus[test_indices]\n",
    "    # Split the time values for each patient\n",
    "    ytime_train, ytime_test = yvtime[train_indices], yvtime[test_indices]\n",
    "\n",
    "    # Scale the input train and test features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    # Create a model to run on the ICGC PDAC (target) data (this uses the source model's weights)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "    model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "    model.add(nnet_survival.PropHazards(n_intervals))\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "    \n",
    "    \n",
    "    if transfer_learning == True:\n",
    "        # Transfer weights of source model's second hidden layer to fine tuning model's second hidden layer\n",
    "        model.layers[1].set_weights(init_model.layers[1].get_weights())\n",
    "\n",
    "\n",
    "    # Create a tensorboard callback which saves the loss during the training of the model\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_fullsetpaad/fold_\" + str(k)), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "    callbacks_list = [tensorboard_callback]\n",
    "\n",
    "    # Fit the model to the input data x_train_scaled and y_train\n",
    "    history = model.fit(x_train_scaled,y_train,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0)\n",
    "\n",
    "\n",
    "    # Make predictions on the train data for the training performance\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_train_scaled,verbose=0), vbreaks, 365)\n",
    "    # Calculate C-index for train data using time, predictions and status values\n",
    "    c_index_train = concordance_index(ytime_train, y_pred, ystatus_train)\n",
    "\n",
    "    # Make predictions on the test data for test performance\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_test_scaled,verbose=0), vbreaks, 365)\n",
    "    # Calculate C-index for test data using time, predictions and status values\n",
    "    c_index_test = concordance_index(ytime_test, y_pred, ystatus_test)\n",
    "\n",
    "    # Store the C-index values in lists to write to .txt files\n",
    "    train_performance.append(c_index_train)\n",
    "    test_performance.append(c_index_test)\n",
    "\n",
    "    total_cindexes.append(c_index_test)\n",
    "    total_cindexes_train.append(c_index_train)\n",
    "\n",
    "    # Save results of current fold to file\n",
    "    with open(results_file, 'a') as o:\n",
    "        print(\"Fold: \" + str(k) + \"\\n\",file=o)\n",
    "        print(\"C-index train set: \" + str(c_index_train), file=o)\n",
    "        print(\"C-index test set: \" + str(c_index_test), file=o)\n",
    "        o.close()\n",
    "\n",
    "    k += 1\n",
    "\n",
    "# Save average C-index values and list of total C-indexes to .txt file\n",
    "with open(results_file, 'a') as o:\n",
    "    print(\"\\n\",file=o)\n",
    "    print(\"Average C-index train total: \" + str(sum(total_cindexes_train) / 50),file=o)\n",
    "    print(\"Average C-index test total: \" + str(sum(total_cindexes) / 50),file=o) \n",
    "    print(str(total_cindexes),file=o)\n",
    "    print(str(total_cindexes_train),file=o)\n",
    "    o.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654d772-02f8-43ee-a3b9-7b7c24e24b2d",
   "metadata": {},
   "source": [
    "### Cox-nnet hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e7aab-d04c-4815-aa28-3cb29fec9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://garmiregroup.org/cox-nnet/docs/examples/ for an example on how to run Cox-nnet, and documentation\n",
    "\n",
    "model_params = dict(node_map = None, input_split = None)\n",
    "search_params = dict(method=\"adam\", learning_rate=0.01, momentum=0.9,\n",
    "                             max_iter=2000, stop_threshold=0.995, patience=1000, patience_incr=2, rand_seed=123,\n",
    "                             eval_step=23, lr_decay=0.9, lr_growth=1.0)\n",
    "cv_params = dict(cv_seed=1, n_folds=5, cv_metric=\"loglikelihood\", L2_range=np.arange(-4.5, 1, 0.5))\n",
    "\n",
    "likelihoods, L2_reg_params, mean_cvpl = cox_nnet.L2CVProfile(xv,yvtime,yvstatus,\n",
    "    model_params, search_params, cv_params, verbose=False)\n",
    "\n",
    "L2_reg = L2_reg_params[np.argmax(mean_cvpl)] #Best L2_reg is -5\n",
    "\n",
    "with open(\"Results/Optimal_L2_coxnnet\", 'a') as o:\n",
    "    print(\"Optimal L2 parameter:\\n\",file=o)\n",
    "    print(L2_reg,file=o)\n",
    "\n",
    "print(L2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4750c-4a6b-46d9-9445-dff647fa5d63",
   "metadata": {},
   "source": [
    "### Comparison Analysis\n",
    "<p> Compare Transfer learning model to Cox-PH and Cox-nnet model </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34fa82-6de9-4964-bb6d-79eacb0db05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = \"Results/Model_Comparison\"\n",
    "\n",
    "# Set optimal parameters found with hyperparameter tuning on TCGA PDAC data\n",
    "\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "# Create a model to initialize the weights of the source model into (should have the same architecture)\n",
    "\n",
    "init_model = Sequential()\n",
    "init_model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "init_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "init_model.add(nnet_survival.PropHazards(10))\n",
    "init_model.compile(loss=nnet_survival.surv_likelihood(10), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Load the weights of the source model into the initialization model\n",
    "\n",
    "init_model.load_weights(\"Weights/source_finetune_model_subset\")\n",
    " \n",
    "\n",
    "# Create Repeated Stratified Kfold cross validation object\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=24)\n",
    "\n",
    "k = 1\n",
    "\n",
    "train_performance_coxph = []\n",
    "test_performance_coxph = []\n",
    "\n",
    "train_performance_coxnnet = []\n",
    "test_performance_coxnnet = []\n",
    "\n",
    "train_performance_transfersnnet = []\n",
    "test_performance_transfersnnet = []\n",
    "\n",
    "# Start the repeated 5 fold cross validation (will repeat 50 times, based on n_splits = 5 and n_repeats = 10)\n",
    "for train_indices, test_indices in rskf.split(xv,yv_final[1]):\n",
    "    print(\"Loop: \" + str(k) + \"/50\")\n",
    "    \n",
    "    # Create splits of the input data\n",
    "    # Split the input features of the ICGC PDAC data\n",
    "    x_train, x_test = xv[train_indices], xv[test_indices]\n",
    "    # Split the input labels of the ICGC PDAC data (survival matrices)\n",
    "    y_train, y_test = np.array(yv_final[0])[train_indices], np.array(yv_final[0])[test_indices]\n",
    "    # Split the time values for each patient\n",
    "    yvtime_train, yvtime_test = yvtime[train_indices], yvtime[test_indices]\n",
    "    # Split the status values for each patient\n",
    "    yvstatus_train, yvstatus_test = yvstatus[train_indices], yvstatus[test_indices]\n",
    "    # Split the custom survival labels used by the cox-ph model\n",
    "    ysurvival_data_train, ysurvival_data_test = ysurvival_data[train_indices], ysurvival_data[test_indices]\n",
    "\n",
    "    \n",
    "    # Scale the input train and test features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    ### Cox-PH ###\n",
    "    \n",
    "    # Create the Cox-PH elastic net model\n",
    "    cox_elastic = CoxnetSurvivalAnalysis(l1_ratio=0.9, alpha_min_ratio=0.01)\n",
    "    \n",
    "    # Fit the model on the scaled train data and custom survival data\n",
    "    cox_elastic.fit(x_train_scaled,ysurvival_data_train)\n",
    "    \n",
    "    # Calculate the C-index on the train data\n",
    "    c_index_train = cox_elastic.score(x_train_scaled,ysurvival_data_train)\n",
    "    # Calculate the C-index on the test data\n",
    "    c_index_test = cox_elastic.score(x_test_scaled,ysurvival_data_test)\n",
    "    \n",
    "    # Add the C-index train score to train_performance_coxph\n",
    "    train_performance_coxph.append(c_index_train)\n",
    "    # Add the C-index test score to test_performance_coxph\n",
    "    test_performance_coxph.append(c_index_test)\n",
    "  \n",
    "    ### Cox-nnet ###\n",
    "\n",
    "    model_params = dict(node_map = None, input_split = None)\n",
    "    search_params = dict(method=\"adam\", learning_rate=0.01, momentum=0.9,\n",
    "                                 max_iter=2000, stop_threshold=0.995, patience=1000, patience_incr=2, rand_seed=123,\n",
    "                                 eval_step=23, lr_decay=0.9, lr_growth=1.0)\n",
    "    cv_params = dict(cv_seed=1, n_folds=5, cv_metric=\"loglikelihood\", L2_range=np.arange(-4.5, 1, 0.5))\n",
    "    \n",
    "    # Train model the model using the optimal L2 parameter found with the hyperparmaeter tuning\n",
    "    L2_reg = -4.5\n",
    "    model_params = dict(node_map = None, input_split = None, L2_reg=np.exp(L2_reg))\n",
    "    cox_nnet_model, cox_nnet_cost_iter = cox_nnet.trainCoxMlp(x_train_scaled, yvtime_train,yvstatus_train, model_params, search_params, verbose=False)\n",
    "    \n",
    "    # Make a prediction on the training data\n",
    "    cox_nnet_theta_train = cox_nnet_model.predictNewData(x_train_scaled)\n",
    "    # Make a prediction on the testing data\n",
    "    cox_nnet_theta_test = cox_nnet_model.predictNewData(x_test_scaled)\n",
    "\n",
    "    # Calculate train C-index from the time data, predictions made by cox-nnet and the patient status data, store the results in train_performance_coxnnet\n",
    "    train_performance_coxnnet.append(concordance_index(yvtime_train,-cox_nnet_theta_train,yvstatus_train))\n",
    "    # Calculate test C-index from the time data, predictions made by cox-nnet and the patient status data, store the results in test_performance_coxnnet\n",
    "    test_performance_coxnnet.append(concordance_index(yvtime_test,-cox_nnet_theta_test,yvstatus_test))\n",
    "\n",
    "    ### OUR MODEL ###\n",
    "    \n",
    "    # Create a model to run on the ICGC PDAC (target) data (this uses the source model's weights)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "    model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "    model.add(nnet_survival.PropHazards(10))\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "    \n",
    "    # Transfer weights of source model's second hidden layer to fine tuning model's second hidden layer\n",
    "    model.layers[1].set_weights(init_model.layers[1].get_weights())\n",
    "\n",
    "    # Create a tensorboard callback which saves the loss during the training of the model\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_comparison_new/fold_\" + str(k)), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "    callbacks_list = [tensorboard_callback]\n",
    "\n",
    "    # Fit the model to the input data x_train_scaled and y_train\n",
    "    history = model.fit(x_train_scaled,y_train,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0)\n",
    "\n",
    "    # Make predictions on the train data for the training performance\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_train_scaled,verbose=0), vbreaks, 365)\n",
    "    # Calculate C-index for train data using time, predictions and status values\n",
    "    c_index_train = concordance_index(yvtime_train, y_pred, yvstatus_train)\n",
    "    \n",
    "    # Make predictions on the test data for test performance\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_test_scaled,verbose=0), vbreaks, 365)\n",
    "    # Calculate C-index for test data using time, predictions and status values\n",
    "    c_index_test = concordance_index(yvtime_test, y_pred, yvstatus_test)\n",
    "\n",
    "    # Save the train C-index of our model to train_performance_transfersnnet\n",
    "    train_performance_transfersnnet.append(c_index_train)\n",
    "    # Save the test C-index of our model to test_performance_transfersnnet\n",
    "    test_performance_transfersnnet.append(c_index_test)\n",
    "    \n",
    "    # Save the average C-index  and total C-index list each approach every fold\n",
    "    with open(results_file, 'a') as o:\n",
    "        print(\"C-indexes, Cox-PH:\\n\",file=o)\n",
    "        print(\"Train:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(train_performance_coxph)/k),file=o)\n",
    "        print(str(train_performance_coxph),file=o)\n",
    "        print(\"Test:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(test_performance_coxph)/k),file=o)\n",
    "        print(str(test_performance_coxph),file=o)\n",
    "        print(\"\\n\",file=o)\n",
    "        print(\"C-indexes, Cox-nnet:\\n\",file=o)\n",
    "        print(\"Train:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(train_performance_coxnnet)/k),file=o)\n",
    "        print(str(train_performance_coxnnet),file=o)\n",
    "        print(\"Test:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(test_performance_coxnnet)/k),file=o)\n",
    "        print(str(test_performance_coxnnet),file=o)\n",
    "        print(\"\\n\",file=o)\n",
    "        print(\"C-indexes, our model:\\n\",file=o)\n",
    "        print(\"Train:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(train_performance_transfersnnet)/k),file=o)\n",
    "        print(str(train_performance_transfersnnet),file=o)\n",
    "        print(\"Test:\\n\",file=o)\n",
    "        print(\"Average C-index: \" + str(sum(test_performance_transfersnnet)/k),file=o)\n",
    "        print(str(test_performance_transfersnnet),file=o)\n",
    "        print(\"\\n\",file=o)\n",
    "    \n",
    "    o.close()\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fd526-7f65-4126-8319-67cfb95f44ab",
   "metadata": {},
   "source": [
    "### Test survival probabilities for patient 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "901fc562-0d2a-477b-bb16-52d9b7837407",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient = xv[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b7ce622-4a20-4751-8c62-a900df885933",
   "metadata": {},
   "outputs": [],
   "source": [
    "xv = np.delete(xv, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57f3aa95-e5e9-4f86-bcaa-fd25cd136f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient_y = yv_final[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01cd2334-2e8b-4a3c-b678-238bcb3a7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = list(yv_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13b55d63-3a3b-4ef4-ae2c-be1f692cc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.delete(y_train_all, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46859b4-39a7-45a4-a9db-74466a77fd5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the file that stores the survival probabilities\n",
    "\n",
    "results_file = \"Results/Model_surv_probabilities\"\n",
    "\n",
    "# Set optimal parameters found with hyperparameter tuning on TCGA PDAC data\n",
    "optimal_l2 = 0.00001\n",
    "optimal_batch = 8\n",
    "\n",
    "# Create a model to initialize the weights of the source model into (should have the same architecture)\n",
    "\n",
    "init_model = Sequential()\n",
    "init_model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "init_model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "init_model.add(nnet_survival.PropHazards(10))\n",
    "init_model.compile(loss=nnet_survival.surv_likelihood(10), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "# Load the weights of the source model into the initialization model\n",
    "\n",
    "init_model.load_weights(\"Weights/source_finetune_model_subset\")\n",
    " \n",
    "\n",
    "# Create Repeated Stratified Kfold cross validation object\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=24)\n",
    "\n",
    "k = 1\n",
    "\n",
    "surv_probablity = []\n",
    "\n",
    "# Set the prediction time you want to get the survival probability of\n",
    "\n",
    "pred_time = 456.25\n",
    "\n",
    "# Start the repeated 5 fold cross validation (will repeat 50 times, based on n_splits = 5 and n_repeats = 10)\n",
    "for _ in range(50):\n",
    "    print(\"Loop: \" + str(k) + \"/50\")\n",
    "    \n",
    "    # Scale the input train and test features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(xv)\n",
    "    x_test_scaled = scaler.transform(test_patient.reshape(1, -1))\n",
    "    # xv_ext_train.iloc[:,0:14242] = scaler.transform(xv_ext_train.iloc[:,0:14242])\n",
    "    # xv_ext_test.iloc[:,0:14242] = scaler.transform(xv_ext_test.iloc[:,0:14242])\n",
    "\n",
    "    # Create a model to run on the ICGC PDAC (target) data (this uses the source model's weights)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(np.sqrt(xv.shape[1]), input_dim=xv.shape[1], bias_initializer='zeros',activation='relu',kernel_regularizer=regularizers.l2(optimal_l2)))\n",
    "    model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "    model.add(nnet_survival.PropHazards(10))\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(learning_rate=0.000001))\n",
    "\n",
    "    \n",
    "    # Transfer weights of source model's second hidden layer to fine tuning model's second hidden layer\n",
    "    model.layers[1].set_weights(init_model.layers[1].get_weights())\n",
    "\n",
    "    # Create a tensorboard callback which saves the loss during the training of the model\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=(\"ktuner/logs_pdac_icgc_testpatient/fold_\" + str(k)), histogram_freq=0, embeddings_freq=0, write_graph=False, update_freq='batch')\n",
    "    callbacks_list = [tensorboard_callback]\n",
    "\n",
    "    # Fit the model to the input data x_train_scaled and y_test_all\n",
    "    history = model.fit(x_train_scaled,y_train_all,batch_size=optimal_batch,epochs=2600, callbacks=callbacks_list,verbose=0)\n",
    "\n",
    "    # Predict the survival probability of patient 5 at set pred_time\n",
    "    y_pred = nnet_survival.nnet_pred_surv(model.predict(x_test_scaled,verbose=0), vbreaks, pred_time)\n",
    "    \n",
    "    # Save survival probability to list\n",
    "    surv_probablity.append(y_pred)\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "# Store survival probabilities to file\n",
    "with open(results_file, 'a') as o:\n",
    "    print(\"Survival probabilities for \" + str(pred_time) + \" days:\\n\", file=o)\n",
    "    print(str(surv_probablity),file=o)\n",
    "    print(str(sum(surv_probablity)/50),file=o)\n",
    "    o.close()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
